{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import cv2\n",
    "\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "from PIL import Image, ImageFilter\n",
    "import io\n",
    "import re\n",
    "import random\n",
    "import numpy.random as npr\n",
    "from skimage import data\n",
    "from scipy.ndimage import rotate\n",
    "from kernels import *\n",
    "import torchvision\n",
    "import os\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    " \n",
    "import my_utils as ut\n",
    "from transformers import Swinv2ForImageClassification, SwinConfig\n",
    "from torch.optim import AdamW\n",
    "from torchvision import transforms, datasets\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "   \n",
    "])\n",
    "\n",
    "# Create dataset instances\n",
    "train_dataset = ut.DatasetAI(root_dir='/mnt/e/GenImage', transform=transform, split='train')\n",
    "val_test_dataset = ut.DatasetAI(root_dir='/mnt/e/GenImage', transform=transform, split='val')\n",
    "\n",
    "\n",
    "train_subset,val_subset, test_subset = ut.split_datasets(train_dataset, val_test_dataset, 300, 300, 300)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_subset, batch_size=32, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_subset, batch_size=32, shuffle=False, num_workers=4)\n",
    "test_loader = DataLoader(test_subset, batch_size=32, shuffle=False, num_workers=4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check overlap between datasets:\n",
      "Train Subset and Validation Subset:\n",
      "No actual data overlap detected.\n",
      "Train Subset and Test Subset:\n",
      "No actual data overlap detected.\n",
      "Validation Subset and Test Subset:\n",
      "No actual data overlap detected.\n",
      "Train Subset Distribution:\n",
      "Total samples in subset: 296\n",
      "Model ADM, Class ai: 20 (6.76%)\n",
      "Model ADM, Class nature: 17 (5.74%)\n",
      "Model BigGAN, Class nature: 20 (6.76%)\n",
      "Model BigGAN, Class ai: 17 (5.74%)\n",
      "Model Midjourney, Class ai: 21 (7.09%)\n",
      "Model Midjourney, Class nature: 16 (5.41%)\n",
      "Model VQDM, Class nature: 11 (3.72%)\n",
      "Model VQDM, Class ai: 26 (8.78%)\n",
      "Model glide, Class nature: 18 (6.08%)\n",
      "Model glide, Class ai: 19 (6.42%)\n",
      "Model stable_diffusion_v_1_4, Class nature: 22 (7.43%)\n",
      "Model stable_diffusion_v_1_4, Class ai: 15 (5.07%)\n",
      "Model stable_diffusion_v_1_5, Class nature: 19 (6.42%)\n",
      "Model stable_diffusion_v_1_5, Class ai: 18 (6.08%)\n",
      "Model wukong, Class ai: 22 (7.43%)\n",
      "Model wukong, Class nature: 15 (5.07%)\n",
      "\n",
      "Validation Subset Distribution:\n",
      "Total samples in subset: 296\n",
      "Model ADM, Class nature: 21 (7.09%)\n",
      "Model ADM, Class ai: 16 (5.41%)\n",
      "Model BigGAN, Class nature: 27 (9.12%)\n",
      "Model BigGAN, Class ai: 10 (3.38%)\n",
      "Model Midjourney, Class nature: 21 (7.09%)\n",
      "Model Midjourney, Class ai: 16 (5.41%)\n",
      "Model VQDM, Class ai: 17 (5.74%)\n",
      "Model VQDM, Class nature: 20 (6.76%)\n",
      "Model glide, Class nature: 19 (6.42%)\n",
      "Model glide, Class ai: 18 (6.08%)\n",
      "Model stable_diffusion_v_1_4, Class nature: 16 (5.41%)\n",
      "Model stable_diffusion_v_1_4, Class ai: 21 (7.09%)\n",
      "Model stable_diffusion_v_1_5, Class nature: 14 (4.73%)\n",
      "Model stable_diffusion_v_1_5, Class ai: 23 (7.77%)\n",
      "Model wukong, Class ai: 20 (6.76%)\n",
      "Model wukong, Class nature: 17 (5.74%)\n",
      "\n",
      "Test Subset Distribution:\n",
      "Total samples in subset: 296\n",
      "Model ADM, Class ai: 20 (6.76%)\n",
      "Model ADM, Class nature: 17 (5.74%)\n",
      "Model BigGAN, Class nature: 19 (6.42%)\n",
      "Model BigGAN, Class ai: 18 (6.08%)\n",
      "Model Midjourney, Class ai: 21 (7.09%)\n",
      "Model Midjourney, Class nature: 16 (5.41%)\n",
      "Model VQDM, Class ai: 21 (7.09%)\n",
      "Model VQDM, Class nature: 16 (5.41%)\n",
      "Model glide, Class ai: 18 (6.08%)\n",
      "Model glide, Class nature: 19 (6.42%)\n",
      "Model stable_diffusion_v_1_4, Class nature: 19 (6.42%)\n",
      "Model stable_diffusion_v_1_4, Class ai: 18 (6.08%)\n",
      "Model stable_diffusion_v_1_5, Class ai: 16 (5.41%)\n",
      "Model stable_diffusion_v_1_5, Class nature: 21 (7.09%)\n",
      "Model wukong, Class nature: 18 (6.08%)\n",
      "Model wukong, Class ai: 19 (6.42%)\n"
     ]
    }
   ],
   "source": [
    "#check distribution of classes \n",
    "\n",
    "print(\"Check overlap between datasets:\")\n",
    "print(\"Train Subset and Validation Subset:\")\n",
    "ut.check_data_overlap(train_subset, val_subset)\n",
    "print(\"Train Subset and Test Subset:\")\n",
    "ut.check_data_overlap(train_subset, test_subset)\n",
    "print(\"Validation Subset and Test Subset:\")\n",
    "ut.check_data_overlap(val_subset, test_subset)\n",
    "\n",
    "print(\"Train Subset Distribution:\")\n",
    "ut.print_model_class_distribution(train_dataset, train_subset.indices)\n",
    "print(\"\\nValidation Subset Distribution:\")\n",
    "ut.print_model_class_distribution(val_test_dataset, val_subset.indices)\n",
    "print(\"\\nTest Subset Distribution:\")\n",
    "ut.print_model_class_distribution(val_test_dataset, test_subset.indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class HighPassFilters(nn.Module):\n",
    "    def __init__(self, kernels):\n",
    "        super(HighPassFilters, self).__init__()\n",
    "        # Kernels are a parameter but not trained\n",
    "        self.kernels = nn.Parameter(kernels, requires_grad=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply convolution with padding to maintain output size equal to input size\n",
    "        return F.conv2d(x, self.kernels, padding =2)  # Padding set to 2 to maintain output size\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CNNBlock(nn.Module):\n",
    "   def __init__(self, kernals):\n",
    "       super(CNNBlock, self).__init__()\n",
    "       self.conv = nn.Conv2d(30, 3, kernel_size=1,padding=0)\n",
    "       self.filters = HighPassFilters(kernals)\n",
    "       self.bn = nn.BatchNorm2d(3)\n",
    "       self.htanh = nn.Hardtanh()\n",
    "   def forward(self, x):\n",
    "       x = self.filters(x)\n",
    "       x = self.conv(x)\n",
    "       x = self.bn(x)\n",
    "       x = self.htanh(x)\n",
    "       return x\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageClassificationModel(nn.Module):\n",
    "    def __init__(self,kernels):\n",
    "        super(ImageClassificationModel, self).__init__()\n",
    "        self.feature_combiner = CNNBlock(kernels)\n",
    "        self.feature_combiner2 = CNNBlock(kernels)\n",
    "        config = SwinConfig.from_pretrained('microsoft/swinv2-tiny-patch4-window8-256',num_classes=2)\n",
    "        self.transformer = Swinv2ForImageClassification.from_pretrained(\n",
    "            \"microsoft/swinv2-tiny-patch4-window8-256\",\n",
    "            config=config\n",
    "        )\n",
    "        \n",
    "        self.transformer.classifier = nn.Linear(config.hidden_size, 2) \n",
    "\n",
    " \n",
    "    def forward(self, rich, poor):\n",
    "       \n",
    "        x = self.feature_combiner(rich)\n",
    "        y = self.feature_combiner2(poor)   \n",
    "        feature_difference = x - y\n",
    "        outputs = self.transformer(feature_difference)\n",
    "\n",
    "        return outputs.logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train & Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ut' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m kernels \u001b[38;5;241m=\u001b[39m \u001b[43mut\u001b[49m\u001b[38;5;241m.\u001b[39mapply_high_pass_filter()\n\u001b[1;32m      2\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m model \u001b[38;5;241m=\u001b[39m ImageClassificationModel(kernels)\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ut' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "kernels = ut.apply_high_pass_filter()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = ImageClassificationModel(kernels).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = AdamW([\n",
    "    {'params': model.feature_combiner.parameters(), 'lr': 1e-5,},\n",
    "    {'params': model.feature_combiner2.parameters(), 'lr': 1e-5,},\n",
    "    {'params': model.transformer.parameters(), 'lr': 1e-5,}\n",
    "])\n",
    "# #freeze the transformer\n",
    "# for param in model.transformer.parameters():\n",
    "#     param.requires_grad = False\n",
    "# #unfreeze classifier\n",
    "# for param in model.transformer.classifier.parameters():\n",
    "#     param.requires_grad = True\n",
    "    \n",
    "best_val_accuracy = 0.0\n",
    "\n",
    "#\n",
    "# Try to load previous best model and its best validation accuracy\n",
    "\n",
    "\n",
    "def train_and_validate(model, train_loader, valid_loader, optimizer, \n",
    "                       device, num_epochs,best_model_path):\n",
    "    \n",
    "    try:\n",
    "        checkpoint = torch.load(best_model_path)\n",
    "        best_val_accuracy = checkpoint['best_val_accuracy']\n",
    "        print(\"Loaded previous best model with accuracy:\", best_val_accuracy)\n",
    "    except FileNotFoundError:\n",
    "        best_val_accuracy = float('-inf')\n",
    "        print(\"No saved model found. Starting fresh!\")\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # # Training Phase\n",
    "        model.train()\n",
    "        total_train_loss, total_train, correct_train = 0, 0, 0\n",
    "        for batch in train_loader:\n",
    "            rich, poor, labels, model_names = batch  # Unpack model_names as well\n",
    "            rich = rich.to(device)\n",
    "            poor = poor.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(rich, poor)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_train_loss += loss.item() * labels.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total_train += labels.size(0)\n",
    "            correct_train += (predicted == labels).sum().item()\n",
    "\n",
    "        train_loss = total_train_loss / total_train\n",
    "        train_accuracy = correct_train / total_train\n",
    "\n",
    "        # Validation Phase\n",
    "        model.eval()\n",
    "        total_val_loss, total_val, correct_val = 0, 0, 0\n",
    "        with torch.no_grad():\n",
    "            for batch in valid_loader:\n",
    "                rich, poor, labels, model_names = batch  # Unpack model_names as well\n",
    "                rich = rich.to(device)\n",
    "                poor = poor.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                outputs = model(rich, poor)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                total_val_loss += loss.item() * labels.size(0)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total_val += labels.size(0)\n",
    "                correct_val += (predicted == labels).sum().item()\n",
    "\n",
    "        val_loss = total_val_loss / total_val\n",
    "        val_accuracy = correct_val / total_val\n",
    "\n",
    "        # Print overall validation accuracy\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}\\n,'\n",
    "              f'Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.4f}, '\n",
    "              f'Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.4f}\\n')\n",
    "        \n",
    "        # Check if general accuracy is the best and save\n",
    "        if val_accuracy > best_val_accuracy:\n",
    "            best_val_accuracy_general = val_accuracy\n",
    "            torch.save({'model_state': model.state_dict(),\n",
    "                        'best_val_accuracy': best_val_accuracy_general},\n",
    "                       best_model_path)\n",
    "            print(f\"Saved new best model with accuracy: {best_val_accuracy_general:.4f}\")\n",
    "best_model_path = '/home/kosta/code/School/SentryAI/pth/best_model_newPatching_fixed_subset.pth'\n",
    "train_and_validate(model, train_loader, val_loader, optimizer, device, num_epochs=10, best_model_path=best_model_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type swinv2 to instantiate a model of type swin. This is not supported for all configurations of models and can yield errors.\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/kosta/code/School/SentryAI/pth/best_model_newPatching.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 42\u001b[0m\n\u001b[1;32m     39\u001b[0m         model_accuracy \u001b[38;5;241m=\u001b[39m stats[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcorrect\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m/\u001b[39m stats[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtotal\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     40\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest Accuracy for model \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_accuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 42\u001b[0m \u001b[43mut\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/code/School/SentryAI/my_utils.py:238\u001b[0m, in \u001b[0;36mtest\u001b[0;34m(model, test_loader, device)\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtest\u001b[39m(model, test_loader, device):\n\u001b[1;32m    237\u001b[0m     \u001b[38;5;66;03m# Load the best model\u001b[39;00m\n\u001b[0;32m--> 238\u001b[0m     checkpoint \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/home/kosta/code/School/SentryAI/pth/best_model_newPatching.pth\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    239\u001b[0m     model\u001b[38;5;241m.\u001b[39mload_state_dict(checkpoint[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_state\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m    241\u001b[0m     model\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/serialization.py:998\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m    995\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    996\u001b[0m     pickle_load_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 998\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[1;32m    999\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m   1000\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m   1001\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m   1002\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m   1003\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/serialization.py:445\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    443\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    444\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 445\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    446\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    447\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/serialization.py:426\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    425\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[0;32m--> 426\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/kosta/code/School/SentryAI/pth/best_model_newPatching.pth'"
     ]
    }
   ],
   "source": [
    "kernels = ut.apply_high_pass_filter()   \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = ImageClassificationModel(kernels).to(device)\n",
    "\n",
    "def test(model, test_loader, device):\n",
    "    # Load the best model\n",
    "    checkpoint = torch.load(\"/home/kosta/code/School/SentryAI/pth/best_model_newPre_SWIN_unfrozen.pth\")\n",
    "    model.load_state_dict(checkpoint['model_state'])\n",
    "    \n",
    "    model.eval()\n",
    "    total_test, correct_test = 0, 0\n",
    "    test_accuracy_per_model = defaultdict(lambda: {'correct': 0, 'total': 0})\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            rich, poor, labels, model_names = batch  # Assuming you have model_names\n",
    "            rich = rich.to(device)\n",
    "            poor = poor.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(rich, poor)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total_test += labels.size(0)\n",
    "            correct_test += (predicted == labels).sum().item()\n",
    "\n",
    "            # Collect stats per model just like validation phase\n",
    "            for model_name, pred, true in zip(model_names, predicted, labels):\n",
    "                test_accuracy_per_model[model_name]['total'] += 1\n",
    "                if pred == true:\n",
    "                    test_accuracy_per_model[model_name]['correct'] += 1\n",
    "\n",
    "    test_accuracy = correct_test / total_test\n",
    "    print(f'Test Accuracy: {test_accuracy:.4f}')\n",
    "\n",
    "    # Print per model accuracy\n",
    "    print(\"-------------------------------------------------------------------------\")\n",
    "    print(\"Test Accuracy per model:\")\n",
    "    for model_name, stats in test_accuracy_per_model.items():\n",
    "        model_accuracy = stats['correct'] / stats['total']\n",
    "        print(f\"Test Accuracy for model {model_name}: {model_accuracy:.4f}\")\n",
    "\n",
    "ut.test(model, test_loader, device,\"/home/kosta/code/School/SentryAI/pth/best_model_newPre_SWIN_unfrozen.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
