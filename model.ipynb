{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import cv2\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image, ImageFilter\n",
    "import io\n",
    "from preprocessing import *\n",
    "import random\n",
    "import numpy.random as npr\n",
    "from skimage import data\n",
    "from scipy.ndimage import rotate\n",
    "from kernels import *\n",
    "import torchvision\n",
    "import os\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from torchvision.datasets import ImageFolder\n",
    "import torchvision.transforms as transforms\n",
    "from preprocessing import *\n",
    "from transformers import Swinv2ForImageClassification, SwinConfig\n",
    "from torch.optim import AdamW\n",
    "from torchvision import transforms, datasets\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess_image(image):    \n",
    "    rich, poor = smash_n_reconstruct(image)\n",
    "    rich = apply_high_pass_filter(rich)\n",
    "    poor = apply_high_pass_filter(poor)\n",
    "    return rich, poor\n",
    "\n",
    "\n",
    "class DatasetAI(Dataset):\n",
    "    def __init__(self, root_dir, transform, split='train'):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.split = split  # This can be 'train', 'val', or 'test'\n",
    "        self.samples = []\n",
    "        self.label_count = {'ai': 0, 'nature': 0}\n",
    "\n",
    "        for model in sorted(os.listdir(root_dir)):\n",
    "            model_path = os.path.join(root_dir, model)\n",
    "            if os.path.isdir(model_path):\n",
    "                # Depending on the split, choose the appropriate subdirectory\n",
    "                split_folder = 'train' if split == 'train' else 'val'\n",
    "                data_dir = os.path.join(model_path, f'imagenet_{model.split(\"_\")[0]}', split_folder)\n",
    "                for class_label in ['ai', 'nature']:\n",
    "                    class_path = os.path.join(data_dir, class_label)\n",
    "                    if os.path.exists(class_path):\n",
    "                        for img_name in os.listdir(class_path):\n",
    "                            if img_name.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                                img_path = os.path.join(class_path, img_name)\n",
    "                                self.samples.append((img_path, class_label))\n",
    "                                self.label_count[class_label] += 1\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, class_label = self.samples[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        rich, poor = smash_n_reconstruct(image)  # Assume this function is defined elsewhere\n",
    "        if self.transform:\n",
    "            rich = self.transform(rich)\n",
    "            poor = self.transform(poor)\n",
    "   \n",
    "        label = 0 if class_label == 'ai' else 1\n",
    "        return rich, poor, label\n",
    "\n",
    "def split_val_test_train(dataset_test_valid, dataset_train, train_size, val_size, test_size, seed=42):\n",
    "    rng = npr.default_rng(seed)\n",
    "    total_size_test_valid = len(dataset_test_valid)\n",
    "    total_size_train = len(dataset_train)\n",
    "\n",
    "    indices_test_valid = np.arange(total_size_test_valid)\n",
    "    indices_train = np.arange(total_size_train)\n",
    "\n",
    "    rng.shuffle(indices_test_valid)\n",
    "    rng.shuffle(indices_train)\n",
    "\n",
    "    if val_size + test_size > total_size_test_valid:\n",
    "        raise ValueError(\"Requested sizes for validation and test exceed available data\")\n",
    "    if train_size > total_size_train:\n",
    "        raise ValueError(\"Requested size for train exceeds available data\")\n",
    "\n",
    "    val_indices = indices_test_valid[:val_size]\n",
    "    test_indices = indices_test_valid[val_size:val_size + test_size]\n",
    "    train_indices = indices_train[:train_size]\n",
    "\n",
    "    val_subset = Subset(dataset_test_valid, val_indices)\n",
    "    test_subset = Subset(dataset_test_valid, test_indices)\n",
    "    train_subset = Subset(dataset_train, train_indices)\n",
    "\n",
    "    return train_subset, val_subset, test_subset\n",
    "\n",
    "\n",
    "\n",
    "transform = transforms.Compose([\n",
    "     transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "   \n",
    "])\n",
    "\n",
    "# Create dataset instances\n",
    "train_dataset = DatasetAI(root_dir='/mnt/d/GenImage', transform=transform, split='train')\n",
    "val_test_dataset = DatasetAI(root_dir='/mnt/d/GenImage', transform=transform, split='val')\n",
    "\n",
    "\n",
    "val_dataset, test_dataset ,train_dataset = split_val_test_train(val_test_dataset, train_dataset, 10, 2, 20)\n",
    "\n",
    "# Create DataLoader for each dataset\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class HighPassFilters(nn.Module):\n",
    "    def __init__(self, kernels):\n",
    "        super(HighPassFilters, self).__init__()\n",
    "        # Kernels are a parameter but not trained\n",
    "        self.kernels = nn.Parameter(kernels, requires_grad=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply convolution with padding to maintain output size equal to input size\n",
    "        return F.conv2d(x, self.kernels, padding =2)  # Padding set to 2 to maintain output size\n",
    "    \n",
    "kernels = apply_high_pass_filter()    \n",
    "print(\"Kernel shape:\", kernels.shape)  \n",
    "model = HighPassFilters(kernels)\n",
    "\n",
    "# Initialize the transformation\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Converts the image to a tensor\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Optional: Normalize\n",
    "])\n",
    "\n",
    "# Load and transform the image\n",
    "image_path = '/home/kosta/code/School/SentryAI/sample_images/img1.jpeg'\n",
    "image = Image.open(image_path).convert('RGB')  # Convert image to RGB\n",
    "image_tensor = transform(image).unsqueeze(0)\n",
    "output = model(image_tensor)\n",
    "print(\"Output shape:\", output.shape)  # Should be [1, 30, height, width]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CNNBlock(nn.Module):\n",
    "   def __init__(self, kernals):\n",
    "       super(CNNBlock, self).__init__()\n",
    "       self.conv = nn.Conv2d(30, 3, kernel_size=1,padding=0)\n",
    "       self.filters = HighPassFilters(kernals)\n",
    "       self.bn = nn.BatchNorm2d(3)\n",
    "       self.htanh = nn.Hardtanh()\n",
    "   def forward(self, x):\n",
    "       x = self.filters(x)\n",
    "       x = self.conv(x)\n",
    "       x = self.bn(x)\n",
    "       x = self.htanh(x)\n",
    "       return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageClassificationModel(nn.Module):\n",
    "    def __init__(self,kernels):\n",
    "        super(ImageClassificationModel, self).__init__()\n",
    "        self.feature_combiner = CNNBlock(kernels)\n",
    "        self.feature_combiner2 = CNNBlock(kernels)\n",
    "        config = SwinConfig.from_pretrained('microsoft/swinv2-tiny-patch4-window8-256',num_classes=2)\n",
    "        self.transformer = Swinv2ForImageClassification.from_pretrained(\n",
    "            \"microsoft/swinv2-tiny-patch4-window8-256\",\n",
    "            config=config\n",
    "        )\n",
    "        \n",
    "        self.transformer.classifier = nn.Linear(config.hidden_size, 2) \n",
    "\n",
    " \n",
    "    def forward(self, rich, poor):\n",
    "       \n",
    "        x = self.feature_combiner(rich)\n",
    "        y = self.feature_combiner2(poor)   \n",
    "        feature_difference = x - y\n",
    "        outputs = self.transformer(feature_difference)\n",
    "\n",
    "        return outputs.logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train & Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type swinv2 to instantiate a model of type swin. This is not supported for all configurations of models and can yield errors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final stacked kernels shape: torch.Size([30, 3, 5, 5])\n",
      "ImageClassificationModel(\n",
      "  (feature_combiner): CNNBlock(\n",
      "    (conv): Conv2d(30, 3, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (filters): HighPassFilters()\n",
      "    (bn): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (htanh): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "  )\n",
      "  (feature_combiner2): CNNBlock(\n",
      "    (conv): Conv2d(30, 3, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (filters): HighPassFilters()\n",
      "    (bn): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (htanh): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "  )\n",
      "  (transformer): Swinv2ForImageClassification(\n",
      "    (swinv2): Swinv2Model(\n",
      "      (embeddings): Swinv2Embeddings(\n",
      "        (patch_embeddings): Swinv2PatchEmbeddings(\n",
      "          (projection): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))\n",
      "        )\n",
      "        (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (encoder): Swinv2Encoder(\n",
      "        (layers): ModuleList(\n",
      "          (0): Swinv2Stage(\n",
      "            (blocks): ModuleList(\n",
      "              (0-1): 2 x Swinv2Layer(\n",
      "                (attention): Swinv2Attention(\n",
      "                  (self): Swinv2SelfAttention(\n",
      "                    (continuous_position_bias_mlp): Sequential(\n",
      "                      (0): Linear(in_features=2, out_features=512, bias=True)\n",
      "                      (1): ReLU(inplace=True)\n",
      "                      (2): Linear(in_features=512, out_features=3, bias=False)\n",
      "                    )\n",
      "                    (query): Linear(in_features=96, out_features=96, bias=True)\n",
      "                    (key): Linear(in_features=96, out_features=96, bias=False)\n",
      "                    (value): Linear(in_features=96, out_features=96, bias=True)\n",
      "                    (dropout): Dropout(p=0.0, inplace=False)\n",
      "                  )\n",
      "                  (output): Swinv2SelfOutput(\n",
      "                    (dense): Linear(in_features=96, out_features=96, bias=True)\n",
      "                    (dropout): Dropout(p=0.0, inplace=False)\n",
      "                  )\n",
      "                )\n",
      "                (layernorm_before): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
      "                (drop_path): Swinv2DropPath(p=0.1)\n",
      "                (intermediate): Swinv2Intermediate(\n",
      "                  (dense): Linear(in_features=96, out_features=384, bias=True)\n",
      "                  (intermediate_act_fn): GELUActivation()\n",
      "                )\n",
      "                (output): Swinv2Output(\n",
      "                  (dense): Linear(in_features=384, out_features=96, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (layernorm_after): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
      "              )\n",
      "            )\n",
      "            (downsample): Swinv2PatchMerging(\n",
      "              (reduction): Linear(in_features=384, out_features=192, bias=False)\n",
      "              (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "          )\n",
      "          (1): Swinv2Stage(\n",
      "            (blocks): ModuleList(\n",
      "              (0-1): 2 x Swinv2Layer(\n",
      "                (attention): Swinv2Attention(\n",
      "                  (self): Swinv2SelfAttention(\n",
      "                    (continuous_position_bias_mlp): Sequential(\n",
      "                      (0): Linear(in_features=2, out_features=512, bias=True)\n",
      "                      (1): ReLU(inplace=True)\n",
      "                      (2): Linear(in_features=512, out_features=6, bias=False)\n",
      "                    )\n",
      "                    (query): Linear(in_features=192, out_features=192, bias=True)\n",
      "                    (key): Linear(in_features=192, out_features=192, bias=False)\n",
      "                    (value): Linear(in_features=192, out_features=192, bias=True)\n",
      "                    (dropout): Dropout(p=0.0, inplace=False)\n",
      "                  )\n",
      "                  (output): Swinv2SelfOutput(\n",
      "                    (dense): Linear(in_features=192, out_features=192, bias=True)\n",
      "                    (dropout): Dropout(p=0.0, inplace=False)\n",
      "                  )\n",
      "                )\n",
      "                (layernorm_before): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "                (drop_path): Swinv2DropPath(p=0.1)\n",
      "                (intermediate): Swinv2Intermediate(\n",
      "                  (dense): Linear(in_features=192, out_features=768, bias=True)\n",
      "                  (intermediate_act_fn): GELUActivation()\n",
      "                )\n",
      "                (output): Swinv2Output(\n",
      "                  (dense): Linear(in_features=768, out_features=192, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (layernorm_after): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "              )\n",
      "            )\n",
      "            (downsample): Swinv2PatchMerging(\n",
      "              (reduction): Linear(in_features=768, out_features=384, bias=False)\n",
      "              (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "          )\n",
      "          (2): Swinv2Stage(\n",
      "            (blocks): ModuleList(\n",
      "              (0-5): 6 x Swinv2Layer(\n",
      "                (attention): Swinv2Attention(\n",
      "                  (self): Swinv2SelfAttention(\n",
      "                    (continuous_position_bias_mlp): Sequential(\n",
      "                      (0): Linear(in_features=2, out_features=512, bias=True)\n",
      "                      (1): ReLU(inplace=True)\n",
      "                      (2): Linear(in_features=512, out_features=12, bias=False)\n",
      "                    )\n",
      "                    (query): Linear(in_features=384, out_features=384, bias=True)\n",
      "                    (key): Linear(in_features=384, out_features=384, bias=False)\n",
      "                    (value): Linear(in_features=384, out_features=384, bias=True)\n",
      "                    (dropout): Dropout(p=0.0, inplace=False)\n",
      "                  )\n",
      "                  (output): Swinv2SelfOutput(\n",
      "                    (dense): Linear(in_features=384, out_features=384, bias=True)\n",
      "                    (dropout): Dropout(p=0.0, inplace=False)\n",
      "                  )\n",
      "                )\n",
      "                (layernorm_before): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "                (drop_path): Swinv2DropPath(p=0.1)\n",
      "                (intermediate): Swinv2Intermediate(\n",
      "                  (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
      "                  (intermediate_act_fn): GELUActivation()\n",
      "                )\n",
      "                (output): Swinv2Output(\n",
      "                  (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (layernorm_after): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "              )\n",
      "            )\n",
      "            (downsample): Swinv2PatchMerging(\n",
      "              (reduction): Linear(in_features=1536, out_features=768, bias=False)\n",
      "              (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "          )\n",
      "          (3): Swinv2Stage(\n",
      "            (blocks): ModuleList(\n",
      "              (0-1): 2 x Swinv2Layer(\n",
      "                (attention): Swinv2Attention(\n",
      "                  (self): Swinv2SelfAttention(\n",
      "                    (continuous_position_bias_mlp): Sequential(\n",
      "                      (0): Linear(in_features=2, out_features=512, bias=True)\n",
      "                      (1): ReLU(inplace=True)\n",
      "                      (2): Linear(in_features=512, out_features=24, bias=False)\n",
      "                    )\n",
      "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (key): Linear(in_features=768, out_features=768, bias=False)\n",
      "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (dropout): Dropout(p=0.0, inplace=False)\n",
      "                  )\n",
      "                  (output): Swinv2SelfOutput(\n",
      "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (dropout): Dropout(p=0.0, inplace=False)\n",
      "                  )\n",
      "                )\n",
      "                (layernorm_before): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (drop_path): Swinv2DropPath(p=0.1)\n",
      "                (intermediate): Swinv2Intermediate(\n",
      "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                  (intermediate_act_fn): GELUActivation()\n",
      "                )\n",
      "                (output): Swinv2Output(\n",
      "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (layernorm_after): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (pooler): AdaptiveAvgPool1d(output_size=1)\n",
      "    )\n",
      "    (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Loaded previous best model with accuracy: 0.8\n",
      "Shape after learning block: torch.Size([20, 3, 256, 256]) \n",
      "Shape after learning block: torch.Size([20, 3, 256, 256]) \n",
      "Shape after learning block: torch.Size([10, 3, 256, 256]) \n",
      "Shape after learning block: torch.Size([10, 3, 256, 256]) \n",
      "Epoch 1/10, Train Loss: 0.7205, Train Accuracy: 0.5000, Validation Loss: 0.7006, Validation Accuracy: 0.6000\n",
      "Shape after learning block: torch.Size([20, 3, 256, 256]) \n",
      "Shape after learning block: torch.Size([20, 3, 256, 256]) \n",
      "Shape after learning block: torch.Size([10, 3, 256, 256]) \n",
      "Shape after learning block: torch.Size([10, 3, 256, 256]) \n",
      "Epoch 2/10, Train Loss: 0.7774, Train Accuracy: 0.5500, Validation Loss: 0.7019, Validation Accuracy: 0.7000\n",
      "Shape after learning block: torch.Size([20, 3, 256, 256]) \n",
      "Shape after learning block: torch.Size([20, 3, 256, 256]) \n",
      "Shape after learning block: torch.Size([10, 3, 256, 256]) \n",
      "Shape after learning block: torch.Size([10, 3, 256, 256]) \n",
      "Epoch 3/10, Train Loss: 0.6862, Train Accuracy: 0.6500, Validation Loss: 0.6818, Validation Accuracy: 0.8000\n",
      "Shape after learning block: torch.Size([20, 3, 256, 256]) \n",
      "Shape after learning block: torch.Size([20, 3, 256, 256]) \n",
      "Shape after learning block: torch.Size([10, 3, 256, 256]) \n",
      "Shape after learning block: torch.Size([10, 3, 256, 256]) \n",
      "Epoch 4/10, Train Loss: 0.6713, Train Accuracy: 0.5500, Validation Loss: 0.6550, Validation Accuracy: 0.7000\n",
      "Shape after learning block: torch.Size([20, 3, 256, 256]) \n",
      "Shape after learning block: torch.Size([20, 3, 256, 256]) \n",
      "Shape after learning block: torch.Size([10, 3, 256, 256]) \n",
      "Shape after learning block: torch.Size([10, 3, 256, 256]) \n",
      "Epoch 5/10, Train Loss: 0.7006, Train Accuracy: 0.6500, Validation Loss: 0.6655, Validation Accuracy: 0.7000\n",
      "Shape after learning block: torch.Size([20, 3, 256, 256]) \n",
      "Shape after learning block: torch.Size([20, 3, 256, 256]) \n",
      "Shape after learning block: torch.Size([10, 3, 256, 256]) \n",
      "Shape after learning block: torch.Size([10, 3, 256, 256]) \n",
      "Epoch 6/10, Train Loss: 0.6399, Train Accuracy: 0.7000, Validation Loss: 0.6704, Validation Accuracy: 0.8000\n",
      "Shape after learning block: torch.Size([20, 3, 256, 256]) \n",
      "Shape after learning block: torch.Size([20, 3, 256, 256]) \n",
      "Shape after learning block: torch.Size([10, 3, 256, 256]) \n",
      "Shape after learning block: torch.Size([10, 3, 256, 256]) \n",
      "Epoch 7/10, Train Loss: 0.6500, Train Accuracy: 0.7500, Validation Loss: 0.6970, Validation Accuracy: 0.7000\n",
      "Shape after learning block: torch.Size([20, 3, 256, 256]) \n",
      "Shape after learning block: torch.Size([20, 3, 256, 256]) \n",
      "Shape after learning block: torch.Size([10, 3, 256, 256]) \n",
      "Shape after learning block: torch.Size([10, 3, 256, 256]) \n",
      "Epoch 8/10, Train Loss: 0.6416, Train Accuracy: 0.7000, Validation Loss: 0.6408, Validation Accuracy: 0.8000\n",
      "Shape after learning block: torch.Size([20, 3, 256, 256]) \n",
      "Shape after learning block: torch.Size([20, 3, 256, 256]) \n",
      "Shape after learning block: torch.Size([10, 3, 256, 256]) \n",
      "Shape after learning block: torch.Size([10, 3, 256, 256]) \n",
      "Epoch 9/10, Train Loss: 0.6639, Train Accuracy: 0.7500, Validation Loss: 0.7374, Validation Accuracy: 0.7000\n",
      "Shape after learning block: torch.Size([20, 3, 256, 256]) \n",
      "Shape after learning block: torch.Size([20, 3, 256, 256]) \n",
      "Shape after learning block: torch.Size([10, 3, 256, 256]) \n",
      "Shape after learning block: torch.Size([10, 3, 256, 256]) \n",
      "Epoch 10/10, Train Loss: 0.6942, Train Accuracy: 0.6000, Validation Loss: 0.6824, Validation Accuracy: 0.8000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "kernels = apply_high_pass_filter()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = ImageClassificationModel(kernels).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = AdamW([\n",
    "    {'params': model.feature_combiner.parameters(), 'lr': 1e-5,},\n",
    "    {'params': model.feature_combiner2.parameters(), 'lr': 1e-5,},\n",
    "    {'params': model.transformer.parameters(), 'lr': 1e-6,}\n",
    "])\n",
    "#freeze the transformer\n",
    "\n",
    "# Initialize the best_val_accuracy variable\n",
    "best_val_accuracy = 0.0\n",
    "best_model_path = 'best_model2.pth'\n",
    "\n",
    "# Try to load previous best model and its best validation accuracy\n",
    "try:\n",
    "    checkpoint = torch.load(best_model_path)\n",
    "    model.load_state_dict(checkpoint['model_state'])\n",
    "    best_val_accuracy = checkpoint['best_val_accuracy']\n",
    "    print(\"Loaded previous best model with accuracy:\", best_val_accuracy)\n",
    "except FileNotFoundError:\n",
    "    best_val_accuracy = float('-inf')\n",
    "    print(\"No saved model found. Starting fresh!\")\n",
    "\n",
    "def train_and_validate(model, train_loader, valid_loader, optimizer, device, num_epochs, best_val_accuracy):\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training Phase\n",
    "        model.train()\n",
    "        total_train_loss, total_train, correct_train = 0, 0, 0\n",
    "        for batch in train_loader:\n",
    "            rich, poor, labels = batch\n",
    "            rich = rich.to(device)\n",
    "            poor = poor.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(rich, poor)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_train_loss += loss.item() * labels.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total_train += labels.size(0)\n",
    "            correct_train += (predicted == labels).sum().item()\n",
    "\n",
    "        train_loss = total_train_loss / total_train\n",
    "        train_accuracy = correct_train / total_train\n",
    "\n",
    "        # Validation Phase\n",
    "        model.eval()\n",
    "        total_val_loss, total_val, correct_val = 0, 0, 0\n",
    "        with torch.no_grad():\n",
    "            for rich, poor, labels in valid_loader:\n",
    "                rich = rich.to(device)\n",
    "                poor = poor.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                outputs = model(rich, poor)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                total_val_loss += loss.item() * labels.size(0)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total_val += labels.size(0)\n",
    "                correct_val += (predicted == labels).sum().item()\n",
    "\n",
    "        val_loss = total_val_loss / total_val\n",
    "        val_accuracy = correct_val / total_val\n",
    "\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, '\n",
    "              f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, '\n",
    "              f'Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}')\n",
    "\n",
    "        # Update the best model if current model is better\n",
    "        if val_accuracy > best_val_accuracy:\n",
    "            best_val_accuracy = val_accuracy\n",
    "            torch.save({'model_state': model.state_dict(),\n",
    "                        'best_val_accuracy': best_val_accuracy},\n",
    "                       best_model_path)\n",
    "            print(f\"Saved new best model with accuracy: {best_val_accuracy:.4f}\")\n",
    "        \n",
    "\n",
    "# Assuming the datasets and loaders are correctly set up\n",
    "train_and_validate(model, train_loader, val_loader, optimizer, device, num_epochs=10, best_val_accuracy=best_val_accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = ImageClassificationModel().to(device)\n",
    "def test(model, test_loader, device):\n",
    "    #load the best model\n",
    "    checkpoint = torch.load(\"best_model.pth\")\n",
    "    model.load_state_dict(checkpoint['model_state'])\n",
    "    \n",
    "    model.eval()\n",
    "    total_test, correct_test = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for rich, poor, labels in test_loader:\n",
    "            rich = rich.to(device)\n",
    "            poor = poor.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(rich, poor)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total_test += labels.size(0)\n",
    "            correct_test += (predicted == labels).sum().item()\n",
    "\n",
    "    test_accuracy = correct_test / total_test\n",
    "    print(f'Test Accuracy: {test_accuracy:.4f}')\n",
    "test(model, test_loader, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
