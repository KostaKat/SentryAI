{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import cv2\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image, ImageFilter\n",
    "import io\n",
    "from preprocessing import *\n",
    "import random\n",
    "import numpy.random as npr\n",
    "from skimage import data\n",
    "from scipy.ndimage import rotate\n",
    "from kernels import *\n",
    "import torchvision\n",
    "import os\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from torchvision.datasets import ImageFolder\n",
    "import torchvision.transforms as transforms\n",
    "from preprocessing import *\n",
    "from transformers import Swinv2ForImageClassification, SwinConfig\n",
    "from torch.optim import AdamW\n",
    "from torchvision import transforms, datasets\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess_image(image):    \n",
    "    rich, poor = smash_n_reconstruct(image)\n",
    "    rich = apply_high_pass_filter(rich)\n",
    "    poor = apply_high_pass_filter(poor)\n",
    "    return rich, poor\n",
    "\n",
    "\n",
    "class DatasetAI(Dataset):\n",
    "    def __init__(self, root_dir, transform, split='train'):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.split = split  # This can be 'train', 'val', or 'test'\n",
    "        self.samples = []\n",
    "        self.label_count = {'ai': 0, 'nature': 0}\n",
    "\n",
    "        for model in sorted(os.listdir(root_dir)):\n",
    "            model_path = os.path.join(root_dir, model)\n",
    "            if os.path.isdir(model_path):\n",
    "                # Depending on the split, choose the appropriate subdirectory\n",
    "                split_folder = 'train' if split == 'train' else 'val'\n",
    "                data_dir = os.path.join(model_path, f'imagenet_{model.split(\"_\")[0]}', split_folder)\n",
    "                for class_label in ['ai', 'nature']:\n",
    "                    class_path = os.path.join(data_dir, class_label)\n",
    "                    if os.path.exists(class_path):\n",
    "                        for img_name in os.listdir(class_path):\n",
    "                            if img_name.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                                img_path = os.path.join(class_path, img_name)\n",
    "                                self.samples.append((img_path, class_label))\n",
    "                                self.label_count[class_label] += 1\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, class_label = self.samples[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        rich, poor = smash_n_reconstruct(image)  # Assume this function is defined elsewhere\n",
    "        if self.transform:\n",
    "            rich = self.transform(rich)\n",
    "            poor = self.transform(poor)\n",
    "   \n",
    "        label = 0 if class_label == 'ai' else 1\n",
    "        return rich, poor, label\n",
    "\n",
    "def split_val_test_train(dataset_test_valid, dataset_train, train_size, val_size, test_size, seed=42):\n",
    "    rng = npr.default_rng(seed)\n",
    "    total_size_test_valid = len(dataset_test_valid)\n",
    "    total_size_train = len(dataset_train)\n",
    "\n",
    "    indices_test_valid = np.arange(total_size_test_valid)\n",
    "    indices_train = np.arange(total_size_train)\n",
    "\n",
    "    rng.shuffle(indices_test_valid)\n",
    "    rng.shuffle(indices_train)\n",
    "\n",
    "    if val_size + test_size > total_size_test_valid:\n",
    "        raise ValueError(\"Requested sizes for validation and test exceed available data\")\n",
    "    if train_size > total_size_train:\n",
    "        raise ValueError(\"Requested size for train exceeds available data\")\n",
    "\n",
    "    val_indices = indices_test_valid[:val_size]\n",
    "    test_indices = indices_test_valid[val_size:val_size + test_size]\n",
    "    train_indices = indices_train[:train_size]\n",
    "\n",
    "    val_subset = Subset(dataset_test_valid, val_indices)\n",
    "    test_subset = Subset(dataset_test_valid, test_indices)\n",
    "    train_subset = Subset(dataset_train, train_indices)\n",
    "\n",
    "    return train_subset, val_subset, test_subset\n",
    "\n",
    "\n",
    "\n",
    "transform = transforms.Compose([\n",
    "     transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "   \n",
    "])\n",
    "\n",
    "# Create dataset instances\n",
    "train_dataset = DatasetAI(root_dir='/mnt/e/GenImage', transform=transform, split='train')\n",
    "val_test_dataset = DatasetAI(root_dir='/mnt/e/GenImage', transform=transform, split='val')\n",
    "\n",
    "\n",
    "val_dataset, test_dataset ,train_dataset = split_val_test_train(val_test_dataset, train_dataset, 10, 2, 20)\n",
    "\n",
    "# Create DataLoader for each dataset\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final stacked kernels shape: torch.Size([30, 3, 5, 5])\n",
      "Kernel shape: torch.Size([30, 3, 5, 5])\n",
      "Output shape: torch.Size([1, 30, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "class HighPassFilters(nn.Module):\n",
    "    def __init__(self, kernels):\n",
    "        super(HighPassFilters, self).__init__()\n",
    "        # Kernels are a parameter but not trained\n",
    "        self.kernels = nn.Parameter(kernels, requires_grad=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply convolution with padding to maintain output size equal to input size\n",
    "        return F.conv2d(x, self.kernels, padding =2)  # Padding set to 2 to maintain output size\n",
    "    \n",
    "kernels = apply_high_pass_filter()    \n",
    "print(\"Kernel shape:\", kernels.shape)  \n",
    "model = HighPassFilters(kernels)\n",
    "\n",
    "# Initialize the transformation\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Converts the image to a tensor\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Optional: Normalize\n",
    "])\n",
    "\n",
    "# Load and transform the image\n",
    "image_path = '/home/kosta/code/School/SentryAI/sample_images/img1.jpeg'\n",
    "image = Image.open(image_path).convert('RGB')  # Convert image to RGB\n",
    "image_tensor = transform(image).unsqueeze(0)\n",
    "output = model(image_tensor)\n",
    "print(\"Output shape:\", output.shape)  # Should be [1, 30, height, width]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CNNBlock(nn.Module):\n",
    "   def __init__(self, kernals):\n",
    "       super(CNNBlock, self).__init__()\n",
    "       self.conv = nn.Conv2d(30, 1, kernel_size=1,padding=0)\n",
    "       self.filters = HighPassFilters(kernals)\n",
    "       self.bn = nn.BatchNorm2d(1)\n",
    "       self.htanh = nn.Hardtanh()\n",
    "   def forward(self, x):\n",
    "       x = self.filters(x)\n",
    "       x = self.conv(x)\n",
    "       x = self.bn(x)\n",
    "       x = self.htanh(x)\n",
    "       print(f\"Shape after learning block: {x.shape} \" )\n",
    "       return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageClassificationModel(nn.Module):\n",
    "    def __init__(self,kernels):\n",
    "        super(ImageClassificationModel, self).__init__()\n",
    "        self.feature_combiner = CNNBlock(kernels)\n",
    "        self.feature_combiner2 = CNNBlock(kernels)\n",
    "        config = SwinConfig.from_pretrained('microsoft/swinv2-tiny-patch4-window8-256',num_classes=2)\n",
    "        self.transformer = Swinv2ForImageClassification.from_pretrained(\n",
    "            \"microsoft/swinv2-tiny-patch4-window8-256\",\n",
    "            config=config\n",
    "        )\n",
    "        \n",
    "        self.transformer.classifier = nn.Linear(config.hidden_size, 2) \n",
    "\n",
    " \n",
    "    def forward(self, rich, poor):\n",
    "       \n",
    "        x = self.feature_combiner(rich)\n",
    "        y = self.feature_combiner2(poor)   \n",
    "        feature_difference = x - y\n",
    "        outputs = self.transformer(feature_difference)\n",
    "\n",
    "        return outputs.logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train & Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type swinv2 to instantiate a model of type swin. This is not supported for all configurations of models and can yield errors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final stacked kernels shape: torch.Size([30, 3, 5, 5])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kosta/.local/lib/python3.10/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ImageClassificationModel(\n",
      "  (feature_combiner): CNNBlock(\n",
      "    (conv): Conv2d(30, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (filters): HighPassFilters()\n",
      "    (bn): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (htanh): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "  )\n",
      "  (feature_combiner2): CNNBlock(\n",
      "    (conv): Conv2d(30, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (filters): HighPassFilters()\n",
      "    (bn): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (htanh): Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "  )\n",
      "  (transformer): Swinv2ForImageClassification(\n",
      "    (swinv2): Swinv2Model(\n",
      "      (embeddings): Swinv2Embeddings(\n",
      "        (patch_embeddings): Swinv2PatchEmbeddings(\n",
      "          (projection): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))\n",
      "        )\n",
      "        (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (encoder): Swinv2Encoder(\n",
      "        (layers): ModuleList(\n",
      "          (0): Swinv2Stage(\n",
      "            (blocks): ModuleList(\n",
      "              (0-1): 2 x Swinv2Layer(\n",
      "                (attention): Swinv2Attention(\n",
      "                  (self): Swinv2SelfAttention(\n",
      "                    (continuous_position_bias_mlp): Sequential(\n",
      "                      (0): Linear(in_features=2, out_features=512, bias=True)\n",
      "                      (1): ReLU(inplace=True)\n",
      "                      (2): Linear(in_features=512, out_features=3, bias=False)\n",
      "                    )\n",
      "                    (query): Linear(in_features=96, out_features=96, bias=True)\n",
      "                    (key): Linear(in_features=96, out_features=96, bias=False)\n",
      "                    (value): Linear(in_features=96, out_features=96, bias=True)\n",
      "                    (dropout): Dropout(p=0.0, inplace=False)\n",
      "                  )\n",
      "                  (output): Swinv2SelfOutput(\n",
      "                    (dense): Linear(in_features=96, out_features=96, bias=True)\n",
      "                    (dropout): Dropout(p=0.0, inplace=False)\n",
      "                  )\n",
      "                )\n",
      "                (layernorm_before): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
      "                (drop_path): Swinv2DropPath(p=0.1)\n",
      "                (intermediate): Swinv2Intermediate(\n",
      "                  (dense): Linear(in_features=96, out_features=384, bias=True)\n",
      "                  (intermediate_act_fn): GELUActivation()\n",
      "                )\n",
      "                (output): Swinv2Output(\n",
      "                  (dense): Linear(in_features=384, out_features=96, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (layernorm_after): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
      "              )\n",
      "            )\n",
      "            (downsample): Swinv2PatchMerging(\n",
      "              (reduction): Linear(in_features=384, out_features=192, bias=False)\n",
      "              (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "          )\n",
      "          (1): Swinv2Stage(\n",
      "            (blocks): ModuleList(\n",
      "              (0-1): 2 x Swinv2Layer(\n",
      "                (attention): Swinv2Attention(\n",
      "                  (self): Swinv2SelfAttention(\n",
      "                    (continuous_position_bias_mlp): Sequential(\n",
      "                      (0): Linear(in_features=2, out_features=512, bias=True)\n",
      "                      (1): ReLU(inplace=True)\n",
      "                      (2): Linear(in_features=512, out_features=6, bias=False)\n",
      "                    )\n",
      "                    (query): Linear(in_features=192, out_features=192, bias=True)\n",
      "                    (key): Linear(in_features=192, out_features=192, bias=False)\n",
      "                    (value): Linear(in_features=192, out_features=192, bias=True)\n",
      "                    (dropout): Dropout(p=0.0, inplace=False)\n",
      "                  )\n",
      "                  (output): Swinv2SelfOutput(\n",
      "                    (dense): Linear(in_features=192, out_features=192, bias=True)\n",
      "                    (dropout): Dropout(p=0.0, inplace=False)\n",
      "                  )\n",
      "                )\n",
      "                (layernorm_before): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "                (drop_path): Swinv2DropPath(p=0.1)\n",
      "                (intermediate): Swinv2Intermediate(\n",
      "                  (dense): Linear(in_features=192, out_features=768, bias=True)\n",
      "                  (intermediate_act_fn): GELUActivation()\n",
      "                )\n",
      "                (output): Swinv2Output(\n",
      "                  (dense): Linear(in_features=768, out_features=192, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (layernorm_after): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "              )\n",
      "            )\n",
      "            (downsample): Swinv2PatchMerging(\n",
      "              (reduction): Linear(in_features=768, out_features=384, bias=False)\n",
      "              (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "          )\n",
      "          (2): Swinv2Stage(\n",
      "            (blocks): ModuleList(\n",
      "              (0-5): 6 x Swinv2Layer(\n",
      "                (attention): Swinv2Attention(\n",
      "                  (self): Swinv2SelfAttention(\n",
      "                    (continuous_position_bias_mlp): Sequential(\n",
      "                      (0): Linear(in_features=2, out_features=512, bias=True)\n",
      "                      (1): ReLU(inplace=True)\n",
      "                      (2): Linear(in_features=512, out_features=12, bias=False)\n",
      "                    )\n",
      "                    (query): Linear(in_features=384, out_features=384, bias=True)\n",
      "                    (key): Linear(in_features=384, out_features=384, bias=False)\n",
      "                    (value): Linear(in_features=384, out_features=384, bias=True)\n",
      "                    (dropout): Dropout(p=0.0, inplace=False)\n",
      "                  )\n",
      "                  (output): Swinv2SelfOutput(\n",
      "                    (dense): Linear(in_features=384, out_features=384, bias=True)\n",
      "                    (dropout): Dropout(p=0.0, inplace=False)\n",
      "                  )\n",
      "                )\n",
      "                (layernorm_before): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "                (drop_path): Swinv2DropPath(p=0.1)\n",
      "                (intermediate): Swinv2Intermediate(\n",
      "                  (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
      "                  (intermediate_act_fn): GELUActivation()\n",
      "                )\n",
      "                (output): Swinv2Output(\n",
      "                  (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (layernorm_after): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "              )\n",
      "            )\n",
      "            (downsample): Swinv2PatchMerging(\n",
      "              (reduction): Linear(in_features=1536, out_features=768, bias=False)\n",
      "              (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "          )\n",
      "          (3): Swinv2Stage(\n",
      "            (blocks): ModuleList(\n",
      "              (0-1): 2 x Swinv2Layer(\n",
      "                (attention): Swinv2Attention(\n",
      "                  (self): Swinv2SelfAttention(\n",
      "                    (continuous_position_bias_mlp): Sequential(\n",
      "                      (0): Linear(in_features=2, out_features=512, bias=True)\n",
      "                      (1): ReLU(inplace=True)\n",
      "                      (2): Linear(in_features=512, out_features=24, bias=False)\n",
      "                    )\n",
      "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (key): Linear(in_features=768, out_features=768, bias=False)\n",
      "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (dropout): Dropout(p=0.0, inplace=False)\n",
      "                  )\n",
      "                  (output): Swinv2SelfOutput(\n",
      "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (dropout): Dropout(p=0.0, inplace=False)\n",
      "                  )\n",
      "                )\n",
      "                (layernorm_before): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (drop_path): Swinv2DropPath(p=0.1)\n",
      "                (intermediate): Swinv2Intermediate(\n",
      "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                  (intermediate_act_fn): GELUActivation()\n",
      "                )\n",
      "                (output): Swinv2Output(\n",
      "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (layernorm_after): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (pooler): AdaptiveAvgPool1d(output_size=1)\n",
      "    )\n",
      "    (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "No saved model found. Starting fresh!\n",
      "Shape after learning block: torch.Size([20, 1, 256, 256]) \n",
      "Shape after learning block: torch.Size([20, 1, 256, 256]) \n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Make sure that the channel dimension of the pixel values match with the one set in the configuration.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_8943/2992980703.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;31m# Assuming the datasets and loaders are correctly set up\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m \u001b[0mtrain_and_validate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_val_accuracy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbest_val_accuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_8943/2992980703.py\u001b[0m in \u001b[0;36mtrain_and_validate\u001b[0;34m(model, train_loader, valid_loader, optimizer, device, num_epochs, best_val_accuracy)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrich\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpoor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_8943/2668369115.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, rich, poor)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_combiner2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpoor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mfeature_difference\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_difference\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/swinv2/modeling_swinv2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, pixel_values, head_mask, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1257\u001b[0m         \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_return_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1259\u001b[0;31m         outputs = self.swinv2(\n\u001b[0m\u001b[1;32m   1260\u001b[0m             \u001b[0mpixel_values\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1261\u001b[0m             \u001b[0mhead_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/swinv2/modeling_swinv2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, pixel_values, bool_masked_pos, head_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1059\u001b[0m         \u001b[0mhead_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_head_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdepths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1060\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1061\u001b[0;31m         \u001b[0membedding_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_dimensions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpixel_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool_masked_pos\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbool_masked_pos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1062\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1063\u001b[0m         encoder_outputs = self.encoder(\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/swinv2/modeling_swinv2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, pixel_values, bool_masked_pos)\u001b[0m\n\u001b[1;32m    305\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpixel_values\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool_masked_pos\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBoolTensor\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m     ) -> Tuple[torch.Tensor]:\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_dimensions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpatch_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpixel_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m         \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m         \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/swinv2/modeling_swinv2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, pixel_values)\u001b[0m\n\u001b[1;32m    358\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_channels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpixel_values\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnum_channels\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_channels\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 360\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    361\u001b[0m                 \u001b[0;34m\"Make sure that the channel dimension of the pixel values match with the one set in the configuration.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m             )\n",
      "\u001b[0;31mValueError\u001b[0m: Make sure that the channel dimension of the pixel values match with the one set in the configuration."
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "kernels = apply_high_pass_filter()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = ImageClassificationModel(kernels).to(device)\n",
    "print(model)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = AdamW([\n",
    "    {'params': model.feature_combiner.parameters(), 'lr': 1e-5,},\n",
    "    {'params': model.feature_combiner2.parameters(), 'lr': 1e-5,},\n",
    "    {'params': model.transformer.parameters(), 'lr': 1e-6,}\n",
    "])\n",
    "#freeze the transformer\n",
    "\n",
    "# Initialize the best_val_accuracy variable\n",
    "best_val_accuracy = 0.0\n",
    "best_model_path = 'best_model.pth'\n",
    "\n",
    "# Try to load previous best model and its best validation accuracy\n",
    "try:\n",
    "    checkpoint = torch.load(best_model_path)\n",
    "    model.load_state_dict(checkpoint['model_state'])\n",
    "    best_val_accuracy = checkpoint['best_val_accuracy']\n",
    "    print(\"Loaded previous best model with accuracy:\", best_val_accuracy)\n",
    "except FileNotFoundError:\n",
    "    best_val_accuracy = float('-inf')\n",
    "    print(\"No saved model found. Starting fresh!\")\n",
    "\n",
    "def train_and_validate(model, train_loader, valid_loader, optimizer, device, num_epochs, best_val_accuracy):\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training Phase\n",
    "        model.train()\n",
    "        total_train_loss, total_train, correct_train = 0, 0, 0\n",
    "        for batch in train_loader:\n",
    "            rich, poor, labels = batch\n",
    "            rich = rich.to(device)\n",
    "            poor = poor.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(rich, poor)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_train_loss += loss.item() * labels.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total_train += labels.size(0)\n",
    "            correct_train += (predicted == labels).sum().item()\n",
    "\n",
    "        train_loss = total_train_loss / total_train\n",
    "        train_accuracy = correct_train / total_train\n",
    "\n",
    "        # Validation Phase\n",
    "        model.eval()\n",
    "        total_val_loss, total_val, correct_val = 0, 0, 0\n",
    "        with torch.no_grad():\n",
    "            for rich, poor, labels in valid_loader:\n",
    "                rich = rich.to(device)\n",
    "                poor = poor.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                outputs = model(rich, poor)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                total_val_loss += loss.item() * labels.size(0)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total_val += labels.size(0)\n",
    "                correct_val += (predicted == labels).sum().item()\n",
    "\n",
    "        val_loss = total_val_loss / total_val\n",
    "        val_accuracy = correct_val / total_val\n",
    "\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, '\n",
    "              f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, '\n",
    "              f'Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}')\n",
    "\n",
    "        # Update the best model if current model is better\n",
    "        if val_accuracy > best_val_accuracy:\n",
    "            best_val_accuracy = val_accuracy\n",
    "            torch.save({'model_state': model.state_dict(),\n",
    "                        'best_val_accuracy': best_val_accuracy},\n",
    "                       best_model_path)\n",
    "            print(f\"Saved new best model with accuracy: {best_val_accuracy:.4f}\")\n",
    "        \n",
    "\n",
    "# Assuming the datasets and loaders are correctly set up\n",
    "train_and_validate(model, train_loader, val_loader, optimizer, device, num_epochs=10, best_val_accuracy=best_val_accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = ImageClassificationModel().to(device)\n",
    "def test(model, test_loader, device):\n",
    "    #load the best model\n",
    "    checkpoint = torch.load(\"best_model.pth\")\n",
    "    model.load_state_dict(checkpoint['model_state'])\n",
    "    \n",
    "    model.eval()\n",
    "    total_test, correct_test = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for rich, poor, labels in test_loader:\n",
    "            rich = rich.to(device)\n",
    "            poor = poor.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(rich, poor)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total_test += labels.size(0)\n",
    "            correct_test += (predicted == labels).sum().item()\n",
    "\n",
    "    test_accuracy = correct_test / total_test\n",
    "    print(f'Test Accuracy: {test_accuracy:.4f}')\n",
    "test(model, test_loader, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
