{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import cv2\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image, ImageFilter\n",
    "import io\n",
    "from preprocessing import *\n",
    "import random\n",
    "import numpy.random as npr\n",
    "from skimage import data\n",
    "from scipy.ndimage import rotate\n",
    "from kernels import *\n",
    "import torchvision\n",
    "import os\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from torchvision.datasets import ImageFolder\n",
    "import torchvision.transforms as transforms\n",
    "from preprocessing import *\n",
    "from transformers import Swinv2ForImageClassification, SwinConfig\n",
    "from torch.optim import AdamW\n",
    "from torchvision import transforms, datasets\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DatasetAI(Dataset):\n",
    "    def __init__(self, root_dir, transform, split='train'):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.split = split  # This can be 'train', 'val', or 'test'\n",
    "        self.samples = []\n",
    "        \n",
    "        self.label_count = {'ai': 0, 'nature': 0}\n",
    "\n",
    "        for model in sorted(os.listdir(root_dir)):\n",
    "            model_path = os.path.join(root_dir, model)\n",
    "            if os.path.isdir(model_path):\n",
    "                # Depending on the split, choose the appropriate subdirectory\n",
    "                split_folder = 'train' if split == 'train' else 'val'\n",
    "                data_dir = os.path.join(model_path, f'imagenet_{model.split(\"_\")[0]}', split_folder)\n",
    "                for class_label in ['ai', 'nature']:\n",
    "                    class_path = os.path.join(data_dir, class_label)\n",
    "                    if os.path.exists(class_path):\n",
    "                        for img_name in os.listdir(class_path):\n",
    "                            if img_name.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                                img_path = os.path.join(class_path, img_name)\n",
    "                                # Include the model name in the sample tuple\n",
    "                                self.samples.append((img_path, class_label, model.split(\"_\")[0]))\n",
    "                                self.label_count[class_label] += 1\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, class_label, model_name = self.samples[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        rich, poor = smash_n_reconstruct(image)  # Assume this function is defined elsewhere\n",
    "        if self.transform:\n",
    "            rich = self.transform(rich)\n",
    "            poor = self.transform(poor)\n",
    "   \n",
    "        label = 0 if class_label == 'ai' else 1\n",
    "        # Return the model name along with the other data\n",
    "        return rich, poor, label, model_name\n",
    "\n",
    "def split_val_test_train(dataset_test_valid, dataset_train, train_size, val_size, test_size, seed=42):\n",
    "    rng = npr.default_rng(seed)\n",
    "    total_size_test_valid = len(dataset_test_valid)\n",
    "    total_size_train = len(dataset_train)\n",
    "\n",
    "    indices_test_valid = np.arange(total_size_test_valid)\n",
    "    indices_train = np.arange(total_size_train)\n",
    "\n",
    "    rng.shuffle(indices_test_valid)\n",
    "    rng.shuffle(indices_train)\n",
    "\n",
    "    if val_size + test_size > total_size_test_valid:\n",
    "        raise ValueError(\"Requested sizes for validation and test exceed available data\")\n",
    "    if train_size > total_size_train:\n",
    "        raise ValueError(\"Requested size for train exceeds available data\")\n",
    "\n",
    "    val_indices = indices_test_valid[:val_size]\n",
    "    test_indices = indices_test_valid[val_size:val_size + test_size]\n",
    "    train_indices = indices_train[:train_size]\n",
    "\n",
    "    val_subset = Subset(dataset_test_valid, val_indices)\n",
    "    test_subset = Subset(dataset_test_valid, test_indices)\n",
    "    train_subset = Subset(dataset_train, train_indices)\n",
    "\n",
    "    return train_subset, val_subset, test_subset\n",
    "\n",
    "\n",
    "\n",
    "transform = transforms.Compose([\n",
    "     transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "   \n",
    "])\n",
    "\n",
    "# Create dataset instances\n",
    "train_dataset = DatasetAI(root_dir='/mnt/d/GenImage', transform=transform, split='train')\n",
    "val_test_dataset = DatasetAI(root_dir='/mnt/d/GenImage', transform=transform, split='val')\n",
    "\n",
    "\n",
    "val_dataset, test_dataset ,train_dataset = split_val_test_train(val_test_dataset, train_dataset, 30000, 6000, 6000)\n",
    "\n",
    "# Create DataLoader for each dataset\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=4)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class HighPassFilters(nn.Module):\n",
    "    def __init__(self, kernels):\n",
    "        super(HighPassFilters, self).__init__()\n",
    "        # Kernels are a parameter but not trained\n",
    "        self.kernels = nn.Parameter(kernels, requires_grad=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply convolution with padding to maintain output size equal to input size\n",
    "        return F.conv2d(x, self.kernels, padding =2)  # Padding set to 2 to maintain output size\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CNNBlock(nn.Module):\n",
    "   def __init__(self, kernals):\n",
    "       super(CNNBlock, self).__init__()\n",
    "       self.conv = nn.Conv2d(30, 3, kernel_size=1,padding=0)\n",
    "       self.filters = HighPassFilters(kernals)\n",
    "       self.bn = nn.BatchNorm2d(3)\n",
    "       self.htanh = nn.Hardtanh()\n",
    "   def forward(self, x):\n",
    "       x = self.filters(x)\n",
    "       x = self.conv(x)\n",
    "       x = self.bn(x)\n",
    "       x = self.htanh(x)\n",
    "       return x\n",
    "   \n",
    "HighPassFilters = HighPassFilters(kernels)\n",
    "model = CNNBlock(kernels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageClassificationModel(nn.Module):\n",
    "    def __init__(self,kernels):\n",
    "        super(ImageClassificationModel, self).__init__()\n",
    "        self.feature_combiner = CNNBlock(kernels)\n",
    "        self.feature_combiner2 = CNNBlock(kernels)\n",
    "        config = SwinConfig.from_pretrained('microsoft/swinv2-tiny-patch4-window8-256',num_classes=2)\n",
    "        self.transformer = Swinv2ForImageClassification.from_pretrained(\n",
    "            \"microsoft/swinv2-tiny-patch4-window8-256\",\n",
    "            config=config\n",
    "        )\n",
    "        \n",
    "        self.transformer.classifier = nn.Linear(config.hidden_size, 2) \n",
    "\n",
    " \n",
    "    def forward(self, rich, poor):\n",
    "       \n",
    "        x = self.feature_combiner(rich)\n",
    "        y = self.feature_combiner2(poor)   \n",
    "        feature_difference = x - y\n",
    "        outputs = self.transformer(feature_difference)\n",
    "\n",
    "        return outputs.logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train & Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "kernels = apply_high_pass_filter()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = ImageClassificationModel(kernels).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = AdamW([\n",
    "    {'params': model.feature_combiner.parameters(), 'lr': 1e-3,},\n",
    "    {'params': model.feature_combiner2.parameters(), 'lr': 1e-3,},\n",
    "    {'params': model.transformer.parameters(), 'lr': 1e-3,}\n",
    "])\n",
    "\n",
    "best_val_accuracy = 0.0\n",
    "best_model_path = 'best_model.pth'\n",
    "\n",
    "# Try to load previous best model and its best validation accuracy\n",
    "try:\n",
    "    checkpoint = torch.load(best_model_path)\n",
    "    model.load_state_dict(checkpoint['model_state'])\n",
    "    best_val_accuracy = checkpoint['best_val_accuracy']\n",
    "    print(\"Loaded previous best model with accuracy:\", best_val_accuracy)\n",
    "except FileNotFoundError:\n",
    "    best_val_accuracy = float('-inf')\n",
    "    print(\"No saved model found. Starting fresh!\")\n",
    "from collections import defaultdict\n",
    "\n",
    "def train_and_validate(model, train_loader, valid_loader, optimizer, device, num_epochs, best_val_accuracy):\n",
    "    best_val_accuracy_per_model = defaultdict(lambda: 0)\n",
    "    best_val_accuracy_general = best_val_accuracy  # Use this to track the overall best accuracy\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # # Training Phase\n",
    "        model.train()\n",
    "        total_train_loss, total_train, correct_train = 0, 0, 0\n",
    "        for batch in train_loader:\n",
    "            rich, poor, labels, model_names = batch  # Unpack model_names as well\n",
    "            rich = rich.to(device)\n",
    "            poor = poor.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(rich, poor)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_train_loss += loss.item() * labels.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total_train += labels.size(0)\n",
    "            correct_train += (predicted == labels).sum().item()\n",
    "\n",
    "        train_loss = total_train_loss / total_train\n",
    "        train_accuracy = correct_train / total_train\n",
    "\n",
    "        # Validation Phase\n",
    "        model.eval()\n",
    "        val_accuracy_per_model = defaultdict(lambda: {'correct': 0, 'total': 0})\n",
    "        total_val_loss, total_val, correct_val = 0, 0, 0\n",
    "        with torch.no_grad():\n",
    "            for batch in valid_loader:\n",
    "                rich, poor, labels, model_names = batch  # Unpack model_names as well\n",
    "                rich = rich.to(device)\n",
    "                poor = poor.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                outputs = model(rich, poor)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                total_val_loss += loss.item() * labels.size(0)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total_val += labels.size(0)\n",
    "                correct_val += (predicted == labels).sum().item()\n",
    "\n",
    "                # Collect stats per model\n",
    "                for model_name, pred, true in zip(model_names, predicted, labels):\n",
    "                    val_accuracy_per_model[model_name]['total'] += 1\n",
    "                    if pred == true:\n",
    "                        val_accuracy_per_model[model_name]['correct'] += 1\n",
    "\n",
    "        val_loss = total_val_loss / total_val\n",
    "        val_accuracy_general = correct_val / total_val\n",
    "\n",
    "        # Print overall validation accuracy\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}\\n,'\n",
    "              f'Train Loss: {\"train_loss:.4f\"}, Train Accuracy: {train_accuracy:.4f},\\n '\n",
    "              f'Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy_general:.4f}\\n')\n",
    "        print(\"-------------------------------------------------------------------------\")\n",
    "        print(\"Validation Accuracy per model:\")\n",
    "        # Print and save best model per type\n",
    "        for model_name, stats in val_accuracy_per_model.items():\n",
    "            model_accuracy = stats['correct'] / stats['total']\n",
    "            print(f\"Validation Accuracy for model {model_name}: {model_accuracy:.4f}\")\n",
    "        \n",
    "        print(\"-------------------------------------------------------------------------\")\n",
    "\n",
    "\n",
    "        # Check if general accuracy is the best and save\n",
    "        if val_accuracy_general > best_val_accuracy_general:\n",
    "            best_val_accuracy_general = val_accuracy_general\n",
    "            torch.save({'model_state': model.state_dict(),\n",
    "                        'best_val_accuracy': best_val_accuracy_general},\n",
    "                       best_model_path)\n",
    "            print(f\"Saved new best general model with accuracy: {best_val_accuracy_general:.4f}\")\n",
    "\n",
    "train_and_validate(model, train_loader, val_loader, optimizer, device, num_epochs=10, best_val_accuracy=best_val_accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = ImageClassificationModel().to(device)\n",
    "def test(model, test_loader, device):\n",
    "    #load the best model\n",
    "    checkpoint = torch.load(\"best_model.pth\")\n",
    "    model.load_state_dict(checkpoint['model_state'])\n",
    "    \n",
    "    model.eval()\n",
    "    total_test, correct_test = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for rich, poor, labels in test_loader:\n",
    "            rich = rich.to(device)\n",
    "            poor = poor.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(rich, poor)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total_test += labels.size(0)\n",
    "            correct_test += (predicted == labels).sum().item()\n",
    "\n",
    "    test_accuracy = correct_test / total_test\n",
    "    print(f'Test Accuracy: {test_accuracy:.4f}')\n",
    "test(model, test_loader, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
