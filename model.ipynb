{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import cv2\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image, ImageFilter\n",
    "import io\n",
    "from preprocessing import *\n",
    "import random\n",
    "import numpy.random as npr\n",
    "from skimage import data\n",
    "from scipy.ndimage import rotate\n",
    "from kernels import *\n",
    "import torchvision\n",
    "import os\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from torchvision.datasets import ImageFolder\n",
    "import torchvision.transforms as transforms\n",
    "from preprocessing import *\n",
    "from transformers import Swinv2ForImageClassification, SwinConfig\n",
    "from torch.optim import AdamW\n",
    "from torchvision import transforms, datasets\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess_image(image):    \n",
    "    rich, poor = smash_n_reconstruct(image)\n",
    "    rich = apply_high_pass_filter(rich)\n",
    "    poor = apply_high_pass_filter(poor)\n",
    "    return rich, poor\n",
    "\n",
    "\n",
    "class DatasetAI(Dataset):\n",
    "    def __init__(self, root_dir, transform=None, split='train'):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.split = split  # This can be 'train', 'val', or 'test'\n",
    "        self.samples = []\n",
    "        self.label_count = {'ai': 0, 'nature': 0}\n",
    "\n",
    "        for model in sorted(os.listdir(root_dir)):\n",
    "            model_path = os.path.join(root_dir, model)\n",
    "            if os.path.isdir(model_path):\n",
    "                # Depending on the split, choose the appropriate subdirectory\n",
    "                split_folder = 'train' if split == 'train' else 'val'\n",
    "                data_dir = os.path.join(model_path, f'imagenet_{model.split(\"_\")[0]}', split_folder)\n",
    "                for class_label in ['ai', 'nature']:\n",
    "                    class_path = os.path.join(data_dir, class_label)\n",
    "                    if os.path.exists(class_path):\n",
    "                        for img_name in os.listdir(class_path):\n",
    "                            if img_name.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                                img_path = os.path.join(class_path, img_name)\n",
    "                                self.samples.append((img_path, class_label))\n",
    "                                self.label_count[class_label] += 1\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, class_label = self.samples[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        rich, poor = preprocess_image(image)  # Assume this function is defined elsewhere\n",
    "        if self.transform:\n",
    "            rich = self.transform(rich)\n",
    "            poor = self.transform(poor)\n",
    "        label = 0 if class_label == 'ai' else 1\n",
    "        return rich, poor, label\n",
    "\n",
    "def split_val_test_train(dataset_test_valid, dataset_train, train_size, val_size, test_size, seed=42):\n",
    "    rng = npr.default_rng(seed)\n",
    "    total_size_test_valid = len(dataset_test_valid)\n",
    "    total_size_train = len(dataset_train)\n",
    "\n",
    "    indices_test_valid = np.arange(total_size_test_valid)\n",
    "    indices_train = np.arange(total_size_train)\n",
    "\n",
    "    rng.shuffle(indices_test_valid)\n",
    "    rng.shuffle(indices_train)\n",
    "\n",
    "    if val_size + test_size > total_size_test_valid:\n",
    "        raise ValueError(\"Requested sizes for validation and test exceed available data\")\n",
    "    if train_size > total_size_train:\n",
    "        raise ValueError(\"Requested size for train exceeds available data\")\n",
    "\n",
    "    val_indices = indices_test_valid[:val_size]\n",
    "    test_indices = indices_test_valid[val_size:val_size + test_size]\n",
    "    train_indices = indices_train[:train_size]\n",
    "\n",
    "    val_subset = Subset(dataset_test_valid, val_indices)\n",
    "    test_subset = Subset(dataset_test_valid, test_indices)\n",
    "    train_subset = Subset(dataset_train, train_indices)\n",
    "\n",
    "    return train_subset, val_subset, test_subset\n",
    "\n",
    "\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Create dataset instances\n",
    "train_dataset = DatasetAI(root_dir='/mnt/d/GenImage', transform=transform, split='train')\n",
    "val_test_dataset = DatasetAI(root_dir='/mnt/d/GenImage', transform=transform, split='val')\n",
    "\n",
    "\n",
    "val_dataset, test_dataset ,train_dataset = split_val_test_train(val_test_dataset, train_dataset, 1000, 200, 200)\n",
    "\n",
    "# Create DataLoader for each dataset\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CNNBlock(nn.Module):\n",
    "   def __init__(self, num_input_channels):\n",
    "       super(CNNBlock, self).__init__()\n",
    "       self.conv = nn.Conv2d(num_input_channels, 3, kernel_size=3, padding=1)\n",
    "       self.bn = nn.BatchNorm2d(3)\n",
    "       self.relu = nn.ReLU()\n",
    "   def forward(self, x):\n",
    "       x = self.conv(x)\n",
    "       x = self.bn(x)\n",
    "       x = self.relu(x)\n",
    "      \n",
    "       return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageClassificationModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ImageClassificationModel, self).__init__()\n",
    "        self.feature_combiner = CNNBlock(num_input_channels=3)\n",
    "        self.feature_combiner2 = CNNBlock(num_input_channels=3)\n",
    "        config = SwinConfig.from_pretrained('microsoft/swinv2-tiny-patch4-window8-256',num_classes=2)\n",
    "        self.transformer = Swinv2ForImageClassification.from_pretrained(\n",
    "            \"microsoft/swinv2-tiny-patch4-window8-256\",\n",
    "            config=config\n",
    "        )\n",
    "        \n",
    "        self.transformer.classifier = nn.Linear(config.hidden_size, 2) \n",
    "\n",
    " \n",
    "    def forward(self, rich, poor):\n",
    "       \n",
    "        x = self.feature_combiner(rich)\n",
    "        y = self.feature_combiner2(poor)   \n",
    "        feature_difference = x - y\n",
    "        outputs = self.transformer(feature_difference)\n",
    "\n",
    "        return outputs.logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train & Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type swinv2 to instantiate a model of type swin. This is not supported for all configurations of models and can yield errors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded previous best model with accuracy: 0.482\n",
      "Epoch 1/10, Train Loss: 0.7179, Train Accuracy: 0.4800, Validation Loss: 0.7136, Validation Accuracy: 0.4720\n",
      "Epoch 2/10, Train Loss: 0.7031, Train Accuracy: 0.5100, Validation Loss: 0.7150, Validation Accuracy: 0.4730\n",
      "Epoch 3/10, Train Loss: 0.7058, Train Accuracy: 0.4900, Validation Loss: 0.7201, Validation Accuracy: 0.4710\n",
      "Epoch 4/10, Train Loss: 0.7047, Train Accuracy: 0.4600, Validation Loss: 0.7205, Validation Accuracy: 0.4700\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = ImageClassificationModel().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = AdamW([\n",
    "    {'params': model.feature_combiner.parameters(), 'lr': 1e-5,},\n",
    "    {'params': model.feature_combiner2.parameters(), 'lr': 1e-5,},\n",
    "    {'params': model.transformer.parameters(), 'lr': 1e-6,}\n",
    "])\n",
    "#freeze the transformer\n",
    "\n",
    "# Initialize the best_val_accuracy variable\n",
    "best_val_accuracy = 0.0\n",
    "best_model_path = 'best_model.pth'\n",
    "\n",
    "# Try to load previous best model and its best validation accuracy\n",
    "try:\n",
    "    checkpoint = torch.load(best_model_path)\n",
    "    model.load_state_dict(checkpoint['model_state'])\n",
    "    best_val_accuracy = checkpoint['best_val_accuracy']\n",
    "    print(\"Loaded previous best model with accuracy:\", best_val_accuracy)\n",
    "except FileNotFoundError:\n",
    "    best_val_accuracy = float('-inf')\n",
    "    print(\"No saved model found. Starting fresh!\")\n",
    "\n",
    "def train_and_validate(model, train_loader, valid_loader, optimizer, device, num_epochs, best_val_accuracy):\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training Phase\n",
    "        model.train()\n",
    "        total_train_loss, total_train, correct_train = 0, 0, 0\n",
    "        for batch in train_loader:\n",
    "            rich, poor, labels = batch\n",
    "            rich = rich.to(device)\n",
    "            poor = poor.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(rich, poor)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_train_loss += loss.item() * labels.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total_train += labels.size(0)\n",
    "            correct_train += (predicted == labels).sum().item()\n",
    "\n",
    "        train_loss = total_train_loss / total_train\n",
    "        train_accuracy = correct_train / total_train\n",
    "\n",
    "        # Validation Phase\n",
    "        model.eval()\n",
    "        total_val_loss, total_val, correct_val = 0, 0, 0\n",
    "        with torch.no_grad():\n",
    "            for rich, poor, labels in valid_loader:\n",
    "                rich = rich.to(device)\n",
    "                poor = poor.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                outputs = model(rich, poor)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                total_val_loss += loss.item() * labels.size(0)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total_val += labels.size(0)\n",
    "                correct_val += (predicted == labels).sum().item()\n",
    "\n",
    "        val_loss = total_val_loss / total_val\n",
    "        val_accuracy = correct_val / total_val\n",
    "\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, '\n",
    "              f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, '\n",
    "              f'Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}')\n",
    "\n",
    "        # Update the best model if current model is better\n",
    "        if val_accuracy > best_val_accuracy:\n",
    "            best_val_accuracy = val_accuracy\n",
    "            torch.save({'model_state': model.state_dict(),\n",
    "                        'best_val_accuracy': best_val_accuracy},\n",
    "                       best_model_path)\n",
    "            print(f\"Saved new best model with accuracy: {best_val_accuracy:.4f}\")\n",
    "        \n",
    "\n",
    "# Assuming the datasets and loaders are correctly set up\n",
    "train_and_validate(model, train_loader, val_loader, optimizer, device, num_epochs=10, best_val_accuracy=best_val_accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type swinv2 to instantiate a model of type swin. This is not supported for all configurations of models and can yield errors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.4150\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = ImageClassificationModel().to(device)\n",
    "def test(model, test_loader, device):\n",
    "    #load the best model\n",
    "    checkpoint = torch.load(\"best_model.pth\")\n",
    "    model.load_state_dict(checkpoint['model_state'])\n",
    "    \n",
    "    model.eval()\n",
    "    total_test, correct_test = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for rich, poor, labels in test_loader:\n",
    "            rich = rich.to(device)\n",
    "            poor = poor.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(rich, poor)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total_test += labels.size(0)\n",
    "            correct_test += (predicted == labels).sum().item()\n",
    "\n",
    "    test_accuracy = correct_test / total_test\n",
    "    print(f'Test Accuracy: {test_accuracy:.4f}')\n",
    "test(model, test_loader, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
