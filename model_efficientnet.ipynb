{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "na-ac-yCub9q",
        "outputId": "c0eea3cc-3c23-44b3-bd1e-c2a752d5d2cf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting timm\n",
            "  Obtaining dependency information for timm from https://files.pythonhosted.org/packages/68/99/2018622d268f6017ddfa5ee71f070bad5d07590374793166baa102849d17/timm-0.9.16-py3-none-any.whl.metadata\n",
            "  Downloading timm-0.9.16-py3-none-any.whl.metadata (38 kB)\n",
            "Collecting torch (from timm)\n",
            "  Obtaining dependency information for torch from https://files.pythonhosted.org/packages/96/23/18b9c16c18a77755e7f15173821c7100f11e6b3b7717bea8d729bdeb92c0/torch-2.2.2-cp311-none-macosx_11_0_arm64.whl.metadata\n",
            "  Downloading torch-2.2.2-cp311-none-macosx_11_0_arm64.whl.metadata (25 kB)\n",
            "Collecting torchvision (from timm)\n",
            "  Obtaining dependency information for torchvision from https://files.pythonhosted.org/packages/36/15/c48f74f8f8d382677ef016b65f09969028a1549b8a518c18894deb95b544/torchvision-0.17.2-cp311-cp311-macosx_11_0_arm64.whl.metadata\n",
            "  Downloading torchvision-0.17.2-cp311-cp311-macosx_11_0_arm64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: pyyaml in /Users/jereenvalsson/anaconda3/lib/python3.11/site-packages (from timm) (6.0)\n",
            "Requirement already satisfied: huggingface_hub in /Users/jereenvalsson/anaconda3/lib/python3.11/site-packages (from timm) (0.15.1)\n",
            "Requirement already satisfied: safetensors in /Users/jereenvalsson/anaconda3/lib/python3.11/site-packages (from timm) (0.3.2)\n",
            "Requirement already satisfied: filelock in /Users/jereenvalsson/anaconda3/lib/python3.11/site-packages (from huggingface_hub->timm) (3.13.1)\n",
            "Requirement already satisfied: fsspec in /Users/jereenvalsson/anaconda3/lib/python3.11/site-packages (from huggingface_hub->timm) (2023.4.0)\n",
            "Requirement already satisfied: requests in /Users/jereenvalsson/anaconda3/lib/python3.11/site-packages (from huggingface_hub->timm) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /Users/jereenvalsson/anaconda3/lib/python3.11/site-packages (from huggingface_hub->timm) (4.65.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/jereenvalsson/anaconda3/lib/python3.11/site-packages (from huggingface_hub->timm) (4.7.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /Users/jereenvalsson/anaconda3/lib/python3.11/site-packages (from huggingface_hub->timm) (23.1)\n",
            "Collecting typing-extensions>=3.7.4.3 (from huggingface_hub->timm)\n",
            "  Obtaining dependency information for typing-extensions>=3.7.4.3 from https://files.pythonhosted.org/packages/01/f3/936e209267d6ef7510322191003885de524fc48d1b43269810cd589ceaf5/typing_extensions-4.11.0-py3-none-any.whl.metadata\n",
            "  Downloading typing_extensions-4.11.0-py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: sympy in /Users/jereenvalsson/anaconda3/lib/python3.11/site-packages (from torch->timm) (1.11.1)\n",
            "Requirement already satisfied: networkx in /Users/jereenvalsson/anaconda3/lib/python3.11/site-packages (from torch->timm) (3.1)\n",
            "Requirement already satisfied: jinja2 in /Users/jereenvalsson/anaconda3/lib/python3.11/site-packages (from torch->timm) (3.1.2)\n",
            "Requirement already satisfied: numpy in /Users/jereenvalsson/anaconda3/lib/python3.11/site-packages (from torchvision->timm) (1.24.3)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/jereenvalsson/anaconda3/lib/python3.11/site-packages (from torchvision->timm) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /Users/jereenvalsson/anaconda3/lib/python3.11/site-packages (from jinja2->torch->timm) (2.1.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/jereenvalsson/anaconda3/lib/python3.11/site-packages (from requests->huggingface_hub->timm) (2.0.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Users/jereenvalsson/anaconda3/lib/python3.11/site-packages (from requests->huggingface_hub->timm) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/jereenvalsson/anaconda3/lib/python3.11/site-packages (from requests->huggingface_hub->timm) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Users/jereenvalsson/anaconda3/lib/python3.11/site-packages (from requests->huggingface_hub->timm) (2023.7.22)\n",
            "Requirement already satisfied: mpmath>=0.19 in /Users/jereenvalsson/anaconda3/lib/python3.11/site-packages (from sympy->torch->timm) (1.3.0)\n",
            "Downloading timm-0.9.16-py3-none-any.whl (2.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading torch-2.2.2-cp311-none-macosx_11_0_arm64.whl (59.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.7/59.7 MB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading torchvision-0.17.2-cp311-cp311-macosx_11_0_arm64.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading typing_extensions-4.11.0-py3-none-any.whl (34 kB)\n",
            "Installing collected packages: typing-extensions, torch, torchvision, timm\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.7.1\n",
            "    Uninstalling typing_extensions-4.7.1:\n",
            "      Successfully uninstalled typing_extensions-4.7.1\n",
            "Successfully installed timm-0.9.16 torch-2.2.2 torchvision-0.17.2 typing-extensions-4.11.0\n"
          ]
        }
      ],
      "source": [
        "!pip install timm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "45SO-ShDPtnp"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'cv2'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[2], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mF\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcv2\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtimm\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'cv2'"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import cv2\n",
        "\n",
        "import timm\n",
        "import numpy as np\n",
        "\n",
        "from PIL import Image, ImageFilter\n",
        "import io\n",
        "import random\n",
        "import numpy.random as npr\n",
        "from skimage import data\n",
        "from scipy.ndimage import rotate\n",
        "import torchvision\n",
        "import os\n",
        "from torchvision.transforms.functional import to_pil_image\n",
        "from torch.utils.data import Dataset, DataLoader, Subset\n",
        "from torchvision.datasets import ImageFolder\n",
        "import torchvision.transforms as transforms\n",
        "from transformers import Swinv2ForImageClassification, SwinConfig\n",
        "from torch.optim import AdamW\n",
        "from torchvision import transforms, datasets\n",
        "from preprocessing import smash_n_reconstruct, apply_high_pass_filter\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "lHLJwTVpQNfk"
      },
      "outputs": [],
      "source": [
        "from torchvision.transforms import Compose, Resize, ToTensor, Normalize\n",
        "\n",
        "# Define the image transformations\n",
        "transformations = Compose([\n",
        "    Resize((224, 224)),  # EfficientNet typically uses 224x224 inputs\n",
        "    ToTensor(),\n",
        "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "def preprocess_image(image):\n",
        "    # Convert image to PIL if it's not already\n",
        "    if not isinstance(image, Image.Image):\n",
        "        image = Image.fromarray(image)\n",
        "\n",
        "    # Apply smash and reconstruct and high pass filter\n",
        "    rich, poor = smash_n_reconstruct(image)\n",
        "    rich = apply_high_pass_filter(rich)\n",
        "    poor = apply_high_pass_filter(poor)\n",
        "\n",
        "    # Apply the resize, to tensor, and normalization transforms\n",
        "    rich = transformations(rich)\n",
        "    poor = transformations(poor)\n",
        "\n",
        "    return rich, poor\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "spj24sU8Tk0p"
      },
      "source": [
        "Preprocessing and Data loader\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "aZYWU2nlRR28"
      },
      "outputs": [],
      "source": [
        "\n",
        "def preprocess_image(image):\n",
        "    rich, poor = smash_n_reconstruct(image)\n",
        "    rich = apply_high_pass_filter(rich)\n",
        "    poor = apply_high_pass_filter(poor)\n",
        "    return rich, poor\n",
        "\n",
        "\n",
        "class DatasetAI(Dataset):\n",
        "    def __init__(self, root_dir, transform=None, split='train'):\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.split = split  # This can be 'train', 'val', or 'test'\n",
        "        self.samples = []\n",
        "        self.label_count = {'ai': 0, 'nature': 0}\n",
        "\n",
        "        for model in sorted(os.listdir(root_dir)):\n",
        "            model_path = os.path.join(root_dir, model)\n",
        "            if os.path.isdir(model_path):\n",
        "                # Depending on the split, choose the appropriate subdirectory\n",
        "                split_folder = 'train' if split == 'train' else 'val'\n",
        "                data_dir = os.path.join(model_path, f'imagenet_{model.split(\"_\")[0]}', split_folder)\n",
        "                for class_label in ['ai', 'nature']:\n",
        "                    class_path = os.path.join(data_dir, class_label)\n",
        "                    if os.path.exists(class_path):\n",
        "                        for img_name in os.listdir(class_path):\n",
        "                            if img_name.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "                                img_path = os.path.join(class_path, img_name)\n",
        "                                self.samples.append((img_path, class_label))\n",
        "                                self.label_count[class_label] += 1\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path, class_label = self.samples[idx]\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        rich, poor = preprocess_image(image)  # Assume this function is defined elsewhere\n",
        "        if self.transform:\n",
        "            rich = self.transform(rich)\n",
        "            poor = self.transform(poor)\n",
        "        label = 0 if class_label == 'ai' else 1\n",
        "        return rich, poor, label\n",
        "\n",
        "def split_val_test_train(dataset_test_valid, dataset_train, train_size, val_size, test_size, seed=42):\n",
        "    rng = npr.default_rng(seed)\n",
        "    total_size_test_valid = len(dataset_test_valid)\n",
        "    total_size_train = len(dataset_train)\n",
        "\n",
        "    indices_test_valid = np.arange(total_size_test_valid)\n",
        "    indices_train = np.arange(total_size_train)\n",
        "\n",
        "    rng.shuffle(indices_test_valid)\n",
        "    rng.shuffle(indices_train)\n",
        "\n",
        "    if val_size + test_size > total_size_test_valid:\n",
        "        raise ValueError(\"Requested sizes for validation and test exceed available data\")\n",
        "    if train_size > total_size_train:\n",
        "        raise ValueError(\"Requested size for train exceeds available data\")\n",
        "\n",
        "    val_indices = indices_test_valid[:val_size]\n",
        "    test_indices = indices_test_valid[val_size:val_size + test_size]\n",
        "    train_indices = indices_train[:train_size]\n",
        "\n",
        "    val_subset = Subset(dataset_test_valid, val_indices)\n",
        "    test_subset = Subset(dataset_test_valid, test_indices)\n",
        "    train_subset = Subset(dataset_train, train_indices)\n",
        "\n",
        "    return train_subset, val_subset, test_subset\n",
        "\n",
        "\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    Resize((224, 224)),  # EfficientNet typically 224x224 inputs\n",
        "    ToTensor(),\n",
        "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "# # Create dataset instances\n",
        "# train_dataset = DatasetAI(root_dir='/mnt/d/GenImage', transform=transform, split='train')\n",
        "# val_test_dataset = DatasetAI(root_dir='/mnt/d/GenImage', transform=transform, split='val')\n",
        "\n",
        "\n",
        "# val_dataset, test_dataset ,train_dataset = split_val_test_train(val_test_dataset, train_dataset, 1000, 200, 200)\n",
        "\n",
        "# # Create DataLoader for each dataset\n",
        "# train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)\n",
        "# val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
        "# test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2yuE-PPwTjEZ"
      },
      "source": [
        "CNN Block"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "OsSI2Z_1TfZk"
      },
      "outputs": [],
      "source": [
        "\n",
        "class CNNBlock(nn.Module):\n",
        "   def __init__(self, num_input_channels):\n",
        "       super(CNNBlock, self).__init__()\n",
        "       self.conv = nn.Conv2d(num_input_channels, 3, kernel_size=3, padding=1)\n",
        "       self.bn = nn.BatchNorm2d(3)\n",
        "       self.relu = nn.ReLU()\n",
        "   def forward(self, x):\n",
        "       x = self.conv(x)\n",
        "       x = self.bn(x)\n",
        "       x = self.relu(x)\n",
        "\n",
        "       return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3LumXp1fYG2J"
      },
      "source": [
        "Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "GStsRmjAUWAd"
      },
      "outputs": [],
      "source": [
        "class ImageClassificationModel_EfficientNet(nn.Module):\n",
        "    def __init__(self, num_classes=2):\n",
        "        super(ImageClassificationModel_EfficientNet, self).__init__()\n",
        "        self.feature_combiner = CNNBlock(num_input_channels=3)\n",
        "        self.feature_combiner2 = CNNBlock(num_input_channels=3)\n",
        "\n",
        "        # Initialize EfficientNet with the desired number of output classes\n",
        "        self.efficientnet = timm.create_model('efficientnet_b0', pretrained=True, num_classes=num_classes)\n",
        "\n",
        "        # Replace the classifier with a dummy since we have our own classifier\n",
        "        self.efficientnet.classifier = nn.Identity()\n",
        "\n",
        "        # Custom classifier that will take the output from EfficientNet\n",
        "        self.classifier = nn.Linear(self.efficientnet.get_classifier().in_features, num_classes)\n",
        "\n",
        "    def forward(self, rich, poor):\n",
        "        # Combine features from 'rich' and 'poor' textures using the CNN blocks\n",
        "        rich_features = self.feature_combiner(rich)\n",
        "        poor_features = self.feature_combiner2(poor)\n",
        "\n",
        "        # Calculate the feature difference\n",
        "        feature_difference = rich_features - poor_features\n",
        "\n",
        "        # Flatten the feature difference\n",
        "        feature_difference = feature_difference.view(feature_difference.size(0), -1)\n",
        "\n",
        "        # Pass the feature difference through EfficientNet to get the features\n",
        "        features = self.efficientnet(feature_difference)\n",
        "\n",
        "        # Pass the features through the custom classifier to get the logits\n",
        "        logits = self.classifier(features)\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 524,
          "referenced_widgets": [
            "6e413c4647424a7daa583e14e2967c82",
            "9f38ac729be94792aaa8c54e93562e2d",
            "7556fd1360314bec9b87ceeac55f65d2",
            "9567877d0f534225ac4696190d90d921",
            "a93844dcf229480984b55a2cf100cd2d",
            "e61ca79dfdb745e7bee8f6bdde32e736",
            "46e395aa5aed4c379548b3d137d6c4f9",
            "77ee2c2114b342c4ac6768257cdec97b",
            "c2d407ad491548a9bfdbc82cb7749699",
            "5b65aca068464a6887f250c002880857",
            "bc196f92bc7642d6b52cd954bb9454c6"
          ]
        },
        "id": "kZRyoXSrYNq4",
        "outputId": "2bb457c4-da7d-4d28-b826-037e982a64b3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6e413c4647424a7daa583e14e2967c82",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/21.4M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "AttributeError",
          "evalue": "'Identity' object has no attribute 'in_features'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-eb54077489b9>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImageClassificationModel_EfficientNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m optimizer = AdamW([\n\u001b[1;32m      5\u001b[0m     \u001b[0;34m{\u001b[0m\u001b[0;34m'params'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_combiner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'lr'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1e-5\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-be204a619de9>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, num_classes)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;31m# Custom classifier that will take the output from EfficientNet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mefficientnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrich\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpoor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1686\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1687\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1688\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"'{type(self).__name__}' object has no attribute '{name}'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1690\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Module'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'Identity' object has no attribute 'in_features'"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = ImageClassificationModel_EfficientNet().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = AdamW([\n",
        "    {'params': model.feature_combiner.parameters(), 'lr': 1e-5},\n",
        "    {'params': model.feature_combiner2.parameters(), 'lr': 1e-5},\n",
        "    {'params': model.efficientnet.parameters(), 'lr': 1e-5},  # To fine-tune EfficientNet\n",
        "    {'params': model.classifier.parameters(), 'lr': 1e-4},\n",
        "])\n",
        "#freeze the transformer\n",
        "\n",
        "# Initialize the best_val_accuracy variable\n",
        "best_val_accuracy = 0.0\n",
        "best_model_path = 'best_model.pth'\n",
        "\n",
        "# Try to load previous best model and its best validation accuracy\n",
        "try:\n",
        "    checkpoint = torch.load(best_model_path)\n",
        "    model.load_state_dict(checkpoint['model_state'])\n",
        "    best_val_accuracy = checkpoint['best_val_accuracy']\n",
        "    print(\"Loaded previous best model with accuracy:\", best_val_accuracy)\n",
        "except FileNotFoundError:\n",
        "    best_val_accuracy = float('-inf')\n",
        "    print(\"No saved model found. Starting fresh!\")\n",
        "\n",
        "def train_and_validate(model, train_loader, valid_loader, optimizer, device, num_epochs, best_val_accuracy):\n",
        "    for epoch in range(num_epochs):\n",
        "        # Training Phase\n",
        "        model.train()\n",
        "        total_train_loss, total_train, correct_train = 0, 0, 0\n",
        "        for batch in train_loader:\n",
        "            rich, poor, labels = batch\n",
        "            rich = rich.to(device)\n",
        "            poor = poor.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(rich, poor)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_train_loss += loss.item() * labels.size(0)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total_train += labels.size(0)\n",
        "            correct_train += (predicted == labels).sum().item()\n",
        "\n",
        "        train_loss = total_train_loss / total_train\n",
        "        train_accuracy = correct_train / total_train\n",
        "\n",
        "        # Validation Phase\n",
        "        model.eval()\n",
        "        total_val_loss, total_val, correct_val = 0, 0, 0\n",
        "        with torch.no_grad():\n",
        "            for rich, poor, labels in valid_loader:\n",
        "                rich = rich.to(device)\n",
        "                poor = poor.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                outputs = model(rich, poor)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                total_val_loss += loss.item() * labels.size(0)\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "                total_val += labels.size(0)\n",
        "                correct_val += (predicted == labels).sum().item()\n",
        "\n",
        "        val_loss = total_val_loss / total_val\n",
        "        val_accuracy = correct_val / total_val\n",
        "\n",
        "        print(f'Epoch {epoch+1}/{num_epochs}, '\n",
        "              f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, '\n",
        "              f'Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}')\n",
        "\n",
        "        # Update the best model if current model is better\n",
        "        if val_accuracy > best_val_accuracy:\n",
        "            best_val_accuracy = val_accuracy\n",
        "            torch.save({'model_state': model.state_dict(),\n",
        "                        'best_val_accuracy': best_val_accuracy},\n",
        "                       best_model_path)\n",
        "            print(f\"Saved new best model with accuracy: {best_val_accuracy:.4f}\")\n",
        "\n",
        "\n",
        "#train_and_validate(model, train_loader, val_loader, optimizer, device, num_epochs=10, best_val_accuracy=best_val_accuracy)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5oqrhTJLu3-L"
      },
      "source": [
        "Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365
        },
        "id": "NQTd1TIXYhz2",
        "outputId": "c7eeb509-8678-4649-ce81-1468ac99cc29"
      },
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "'Identity' object has no attribute 'in_features'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-190d52cc02f8>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImageClassificationModel_EfficientNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m#load the best model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"best_model.pth\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-be204a619de9>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, num_classes)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;31m# Custom classifier that will take the output from EfficientNet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mefficientnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrich\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpoor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1686\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1687\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1688\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"'{type(self).__name__}' object has no attribute '{name}'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1690\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Module'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'Identity' object has no attribute 'in_features'"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = ImageClassificationModel_EfficientNet().to(device)\n",
        "def test(model, test_loader, device):\n",
        "    #load the best model\n",
        "    checkpoint = torch.load(\"best_model.pth\")\n",
        "    model.load_state_dict(checkpoint['model_state'])\n",
        "\n",
        "    model.eval()\n",
        "    total_test, correct_test = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for rich, poor, labels in test_loader:\n",
        "            rich = rich.to(device)\n",
        "            poor = poor.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(rich, poor)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total_test += labels.size(0)\n",
        "            correct_test += (predicted == labels).sum().item()\n",
        "\n",
        "    test_accuracy = correct_test / total_test\n",
        "    print(f'Test Accuracy: {test_accuracy:.4f}')\n",
        "#test(model, test_loader, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "La77zh7yu8PZ"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "46e395aa5aed4c379548b3d137d6c4f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5b65aca068464a6887f250c002880857": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6e413c4647424a7daa583e14e2967c82": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9f38ac729be94792aaa8c54e93562e2d",
              "IPY_MODEL_7556fd1360314bec9b87ceeac55f65d2",
              "IPY_MODEL_9567877d0f534225ac4696190d90d921"
            ],
            "layout": "IPY_MODEL_a93844dcf229480984b55a2cf100cd2d"
          }
        },
        "7556fd1360314bec9b87ceeac55f65d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_77ee2c2114b342c4ac6768257cdec97b",
            "max": 21355344,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c2d407ad491548a9bfdbc82cb7749699",
            "value": 21355344
          }
        },
        "77ee2c2114b342c4ac6768257cdec97b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9567877d0f534225ac4696190d90d921": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5b65aca068464a6887f250c002880857",
            "placeholder": "​",
            "style": "IPY_MODEL_bc196f92bc7642d6b52cd954bb9454c6",
            "value": " 21.4M/21.4M [00:00&lt;00:00, 83.6MB/s]"
          }
        },
        "9f38ac729be94792aaa8c54e93562e2d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e61ca79dfdb745e7bee8f6bdde32e736",
            "placeholder": "​",
            "style": "IPY_MODEL_46e395aa5aed4c379548b3d137d6c4f9",
            "value": "model.safetensors: 100%"
          }
        },
        "a93844dcf229480984b55a2cf100cd2d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bc196f92bc7642d6b52cd954bb9454c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c2d407ad491548a9bfdbc82cb7749699": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e61ca79dfdb745e7bee8f6bdde32e736": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
